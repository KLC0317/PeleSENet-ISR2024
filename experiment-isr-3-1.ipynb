{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":505351,"sourceType":"datasetVersion","datasetId":174469}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport os\nimport glob\nimport time\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, matthews_corrcoef,\n    confusion_matrix, ConfusionMatrixDisplay,\n    classification_report, precision_recall_fscore_support\n)\n\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torch.hub import load_state_dict_from_url\nfrom torchvision import models\n\n\nimport math\nfrom inspect import isfunction\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\", \n    category=UserWarning, \n    module=\"torchvision.models._utils\"\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:20:24.980997Z","iopub.execute_input":"2025-02-04T04:20:24.981204Z","iopub.status.idle":"2025-02-04T04:20:32.838410Z","shell.execute_reply.started":"2025-02-04T04:20:24.981184Z","shell.execute_reply":"2025-02-04T04:20:32.837744Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class CFG:\n    EPOCHS = 30\n    BATCH_SIZE = 32\n    SEED = 3170310\n    HEIGHT = 224\n    WIDTH = 224\n    CHANNELS = 3\n    IMAGE_SIZE = (224, 224, 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:21:03.760254Z","iopub.execute_input":"2025-02-04T04:21:03.760606Z","iopub.status.idle":"2025-02-04T04:21:03.764945Z","shell.execute_reply.started":"2025-02-04T04:21:03.760579Z","shell.execute_reply":"2025-02-04T04:21:03.763979Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def seed_everything(seed=CFG.SEED):\n    random.seed(seed)  # Python random module\n    np.random.seed(seed)  # NumPy\n    torch.manual_seed(seed)  # PyTorch CPU\n    torch.cuda.manual_seed(seed)  # PyTorch GPU\n    torch.cuda.manual_seed_all(seed)  # All GPUs\n    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior\n    torch.backends.cudnn.benchmark = False  # Disable optimization for reproducibility\n\nseed_everything(CFG.SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:21:04.084923Z","iopub.execute_input":"2025-02-04T04:21:04.085305Z","iopub.status.idle":"2025-02-04T04:21:04.096824Z","shell.execute_reply.started":"2025-02-04T04:21:04.085264Z","shell.execute_reply":"2025-02-04T04:21:04.095959Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Define paths\nDATASET_PATH = \"/kaggle/input/skin-cancer-malignant-vs-benign/\"\nTRAIN_PATH = '/kaggle/input/skin-cancer-malignant-vs-benign/train/'\nTEST_PATH = '/kaggle/input/skin-cancer-malignant-vs-benign/test/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:21:05.607747Z","iopub.execute_input":"2025-02-04T04:21:05.608033Z","iopub.status.idle":"2025-02-04T04:21:05.611764Z","shell.execute_reply.started":"2025-02-04T04:21:05.608012Z","shell.execute_reply":"2025-02-04T04:21:05.610911Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"%%time\ntrain_images = glob.glob(f\"{TRAIN_PATH}**/*.jpg\")\ntest_images = glob.glob(f\"{TEST_PATH}**/*.jpg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:21:05.788335Z","iopub.execute_input":"2025-02-04T04:21:05.788608Z","iopub.status.idle":"2025-02-04T04:21:05.888257Z","shell.execute_reply.started":"2025-02-04T04:21:05.788588Z","shell.execute_reply":"2025-02-04T04:21:05.887437Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 7.05 ms, sys: 3.02 ms, total: 10.1 ms\nWall time: 95 ms\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Get train & test set sizes\ntrain_size = len(train_images)\ntest_size = len(test_images)\n\n# Get dataset size\ntotal = train_size + test_size\n\n# View samples counts\nprint(f'train samples count:\\t\\t{train_size}')\nprint(f'test samples count:\\t\\t{test_size}')\nprint('=======================================')\nprint(f'TOTAL:\\t\\t\\t\\t{total}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:21:07.498191Z","iopub.execute_input":"2025-02-04T04:21:07.498482Z","iopub.status.idle":"2025-02-04T04:21:07.504041Z","shell.execute_reply.started":"2025-02-04T04:21:07.498460Z","shell.execute_reply":"2025-02-04T04:21:07.503306Z"}},"outputs":[{"name":"stdout","text":"train samples count:\t\t2637\ntest samples count:\t\t660\n=======================================\nTOTAL:\t\t\t\t3297\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def generate_labels(image_paths):\n    return [_.split('/')[-2:][0] for _ in image_paths]\n\n\ndef build_df(image_paths, labels):\n    # Modified version with proper error checking\n    df = pd.DataFrame({\n        'image_path': image_paths,\n        'label': labels\n    })\n    \n    # Add print statement to verify unique labels\n    print(\"Unique labels before encoding:\", df['label'].unique())\n    \n    # Modified label encoding with value counts\n    df['label_encoded'] = df.apply(lambda row: 0 if row.label == 'malignant' else 1, axis=1)\n    print(\"Label distribution after encoding:\", df['label_encoded'].value_counts())\n    \n    return df.sample(frac=1, random_state=CFG.SEED).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:21:07.669142Z","iopub.execute_input":"2025-02-04T04:21:07.669438Z","iopub.status.idle":"2025-02-04T04:21:07.674824Z","shell.execute_reply.started":"2025-02-04T04:21:07.669415Z","shell.execute_reply":"2025-02-04T04:21:07.674017Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Build the DataFrames\ntrain_df = build_df(train_images, generate_labels(train_images))\ntest_df = build_df(test_images, generate_labels(test_images))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:21:07.826186Z","iopub.execute_input":"2025-02-04T04:21:07.826480Z","iopub.status.idle":"2025-02-04T04:21:07.884770Z","shell.execute_reply.started":"2025-02-04T04:21:07.826457Z","shell.execute_reply":"2025-02-04T04:21:07.884042Z"}},"outputs":[{"name":"stdout","text":"Unique labels before encoding: ['benign' 'malignant']\nLabel distribution after encoding: label_encoded\n1    1440\n0    1197\nName: count, dtype: int64\nUnique labels before encoding: ['benign' 'malignant']\nLabel distribution after encoding: label_encoded\n1    360\n0    300\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Create Train/Val split with Training Set\ntrain_split_idx, val_split_idx, _, _ = train_test_split(train_df.index, \n                                                        train_df.label_encoded, \n                                                        test_size=0.15,\n                                                        stratify=train_df.label_encoded,\n                                                        random_state=CFG.SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:21:08.428091Z","iopub.execute_input":"2025-02-04T04:21:08.428511Z","iopub.status.idle":"2025-02-04T04:21:08.437886Z","shell.execute_reply.started":"2025-02-04T04:21:08.428473Z","shell.execute_reply":"2025-02-04T04:21:08.436993Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Get new training and validation data\ntrain_new_df = train_df.iloc[train_split_idx].reset_index(drop=True)\nval_df = train_df.iloc[val_split_idx].reset_index(drop=True)\n\n# View shapes\ntrain_new_df.shape, val_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:21:10.137093Z","iopub.execute_input":"2025-02-04T04:21:10.137414Z","iopub.status.idle":"2025-02-04T04:21:10.145584Z","shell.execute_reply.started":"2025-02-04T04:21:10.137386Z","shell.execute_reply":"2025-02-04T04:21:10.144672Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"((2241, 3), (396, 3))"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nclass SkinCancerDataset(Dataset):\n    def __init__(self, df, transform=None):\n        \"\"\"\n        Args:\n            df (pandas.DataFrame): DataFrame containing image paths and labels\n            transform (callable, optional): Optional transform to be applied on an image\n        \"\"\"\n        self.df = df\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_path = self.df.iloc[idx]['image_path']\n        label = self.df.iloc[idx]['label_encoded']\n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n\nclass AugmentedSkinCancerDataset(Dataset):\n    def __init__(self, df, transform=None, augment_times=4):\n        self.df = df\n        self.base_transform = transform\n        self.augment_times = augment_times\n        \n        self.augmentations = [\n            transforms.Compose([\n                transforms.RandomChoice([\n                    transforms.RandomRotation(90),\n                    transforms.RandomHorizontalFlip(p=0.8),\n                    transforms.RandomVerticalFlip(p=0.8),\n                ]),\n                transforms.ColorJitter(\n                    brightness=0.2,\n                    contrast=0.2,\n                    saturation=0.2,\n                    hue=0.1\n                ),\n                transforms.RandomAffine(\n                    degrees=30,\n                    translate=(0.1, 0.1),\n                    scale=(0.8, 1.2),\n                    shear=10\n                ),\n                transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n            ]) for _ in range(augment_times)\n        ]\n\n    def __len__(self):\n        return len(self.df) * (self.augment_times + 1)  \n\n    def __getitem__(self, idx):\n        original_idx = idx // (self.augment_times + 1)\n        aug_idx = idx % (self.augment_times + 1)\n        \n        img_path = self.df.iloc[original_idx]['image_path']\n        label = self.df.iloc[original_idx]['label_encoded']\n        image = Image.open(img_path).convert('RGB')\n        \n        image = transforms.Resize((224, 224))(image)\n        \n        if aug_idx == 0:\n            if self.base_transform:\n                image = self.base_transform(image)\n            return image, label\n        \n        image = self.augmentations[aug_idx-1](image)\n        \n        if self.base_transform:\n            image = self.base_transform(image)\n            \n        return image, label\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomVerticalFlip(),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(90),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nbase_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = AugmentedSkinCancerDataset(\n    train_new_df,\n    transform=base_transforms,\n    augment_times=4\n)\nval_dataset = SkinCancerDataset(val_df, transform=val_transforms)\ntest_dataset = SkinCancerDataset(test_df, transform=val_transforms)\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=CFG.BATCH_SIZE, \n    shuffle=True,\n    num_workers=4\n)\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=CFG.BATCH_SIZE, \n    shuffle=False,\n    num_workers=4\n)\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=CFG.BATCH_SIZE, \n    shuffle=False,\n    num_workers=4\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:21:10.408095Z","iopub.execute_input":"2025-02-04T04:21:10.408384Z","iopub.status.idle":"2025-02-04T04:21:10.422344Z","shell.execute_reply.started":"2025-02-04T04:21:10.408360Z","shell.execute_reply":"2025-02-04T04:21:10.421631Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(f\"Original dataset size: {len(train_new_df)}\")\nprint(f\"Augmented dataset size: {len(train_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:21:13.231820Z","iopub.execute_input":"2025-02-04T04:21:13.232134Z","iopub.status.idle":"2025-02-04T04:21:13.236869Z","shell.execute_reply.started":"2025-02-04T04:21:13.232111Z","shell.execute_reply":"2025-02-04T04:21:13.235887Z"}},"outputs":[{"name":"stdout","text":"Original dataset size: 2241\nAugmented dataset size: 11205\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Customised Blocks ","metadata":{}},{"cell_type":"code","source":"def round_channels(channels,\n                   divisor=8):\n    \"\"\"\n    Round weighted channel number (make divisible operation).\n\n    Parameters:\n    ----------\n    channels : int or float\n        Original number of channels.\n    divisor : int, default 8\n        Alignment value.\n\n    Returns\n    -------\n    int\n        Weighted number of channels.\n    \"\"\"\n    rounded_channels = max(int(channels + divisor / 2.0) // divisor * divisor, divisor)\n    if float(rounded_channels) < 0.9 * channels:\n        rounded_channels += divisor\n    return rounded_channels\n\n\nclass Swish(nn.Module):\n    \"\"\"\n    Swish activation function from 'Searching for Activation Functions,' https://arxiv.org/abs/1710.05941.\n    \"\"\"\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\nclass HSigmoid(nn.Module):\n    \"\"\"\n    Approximated sigmoid function, so-called hard-version of sigmoid from 'Searching for MobileNetV3,'\n    https://arxiv.org/abs/1905.02244.\n    \"\"\"\n    def forward(self, x):\n        return F.relu6(x + 3.0, inplace=True) / 6.0\n\n\nclass HSwish(nn.Module):\n    \"\"\"\n    H-Swish activation function from 'Searching for MobileNetV3,' https://arxiv.org/abs/1905.02244.\n\n    Parameters:\n    ----------\n    inplace : bool\n        Whether to use inplace version of the module.\n    \"\"\"\n    def __init__(self, inplace=False):\n        super(HSwish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return x * F.relu6(x + 3.0, inplace=self.inplace) / 6.0\n\n\ndef get_activation_layer(activation):\n    \"\"\"\n    Create activation layer from string/function.\n\n    Parameters:\n    ----------\n    activation : function, or str, or nn.Module\n        Activation function or name of activation function.\n\n    Returns\n    -------\n    nn.Module\n        Activation layer.\n    \"\"\"\n    assert (activation is not None)\n    if isfunction(activation):\n        return activation()\n    elif isinstance(activation, str):\n        if activation == \"relu\":\n            return nn.ReLU(inplace=True)\n        elif activation == \"relu6\":\n            return nn.ReLU6(inplace=True)\n        elif activation == \"swish\":\n            return Swish()\n        elif activation == \"hswish\":\n            return HSwish(inplace=True)\n        elif activation == \"sigmoid\":\n            return nn.Sigmoid()\n        elif activation == \"hsigmoid\":\n            return HSigmoid()\n        else:\n            raise NotImplementedError()\n    else:\n        assert (isinstance(activation, nn.Module))\n        return activation\n\n\ndef conv1x1(in_channels,\n            out_channels,\n            stride=1,\n            groups=1,\n            bias=False):\n    \"\"\"\n    Convolution 1x1 layer.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int, default 1\n        Strides of the convolution.\n    groups : int, default 1\n        Number of groups.\n    bias : bool, default False\n        Whether the layer uses a bias vector.\n    \"\"\"\n    return nn.Conv2d(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=1,\n        stride=stride,\n        groups=groups,\n        bias=bias)\n\n\ndef conv3x3(in_channels,\n            out_channels,\n            stride=1,\n            padding=1,\n            dilation=1,\n            groups=1,\n            bias=False):\n    \"\"\"\n    Convolution 3x3 layer.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int, default 1\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int, default 1\n        Padding value for convolution layer.\n    groups : int, default 1\n        Number of groups.\n    bias : bool, default False\n        Whether the layer uses a bias vector.\n    \"\"\"\n    return nn.Conv2d(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=3,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        groups=groups,\n        bias=bias)\n\n\ndef depthwise_conv3x3(channels,\n                      stride):\n    \"\"\"\n    Depthwise convolution 3x3 layer.\n\n    Parameters:\n    ----------\n    channels : int\n        Number of input/output channels.\n    strides : int or tuple/list of 2 int\n        Strides of the convolution.\n    \"\"\"\n    return nn.Conv2d(\n        in_channels=channels,\n        out_channels=channels,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        groups=channels,\n        bias=False)\n\n\nclass ConvBlock(nn.Module):\n    \"\"\"\n    Standard convolution block with Batch normalization and activation.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    kernel_size : int or tuple/list of 2 int\n        Convolution window size.\n    stride : int or tuple/list of 2 int\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int\n        Padding value for convolution layer.\n    dilation : int or tuple/list of 2 int, default 1\n        Dilation value for convolution layer.\n    groups : int, default 1\n        Number of groups.\n    bias : bool, default False\n        Whether the layer uses a bias vector.\n    use_bn : bool, default True\n        Whether to use BatchNorm layer.\n    bn_eps : float, default 1e-5\n        Small float added to variance in Batch norm.\n    activation : function or str or None, default nn.ReLU(inplace=True)\n        Activation function or name of activation function.\n    \"\"\"\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride,\n                 padding,\n                 dilation=1,\n                 groups=1,\n                 bias=False,\n                 use_bn=True,\n                 bn_eps=1e-5,\n                 activation=(lambda: nn.ReLU(inplace=True))):\n        super(ConvBlock, self).__init__()\n        self.activate = (activation is not None)\n        self.use_bn = use_bn\n\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias)\n        if self.use_bn:\n            self.bn = nn.BatchNorm2d(\n                num_features=out_channels,\n                eps=bn_eps)\n        if self.activate:\n            self.activ = get_activation_layer(activation)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.use_bn:\n            x = self.bn(x)\n        if self.activate:\n            x = self.activ(x)\n        return x\n\n\ndef conv1x1_block(in_channels,\n                  out_channels,\n                  stride=1,\n                  padding=0,\n                  groups=1,\n                  bias=False,\n                  use_bn=True,\n                  bn_eps=1e-5,\n                  activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"\n    1x1 version of the standard convolution block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int, default 1\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int, default 0\n        Padding value for convolution layer.\n    groups : int, default 1\n        Number of groups.\n    bias : bool, default False\n        Whether the layer uses a bias vector.\n    use_bn : bool, default True\n        Whether to use BatchNorm layer.\n    bn_eps : float, default 1e-5\n        Small float added to variance in Batch norm.\n    activation : function or str or None, default nn.ReLU(inplace=True)\n        Activation function or name of activation function.\n    \"\"\"\n    return ConvBlock(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=1,\n        stride=stride,\n        padding=padding,\n        groups=groups,\n        bias=bias,\n        use_bn=use_bn,\n        bn_eps=bn_eps,\n        activation=activation)\n\n\ndef conv3x3_block(in_channels,\n                  out_channels,\n                  stride=1,\n                  padding=1,\n                  dilation=1,\n                  groups=1,\n                  bias=False,\n                  use_bn=True,\n                  bn_eps=1e-5,\n                  activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"\n    3x3 version of the standard convolution block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int, default 1\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int, default 1\n        Padding value for convolution layer.\n    dilation : int or tuple/list of 2 int, default 1\n        Dilation value for convolution layer.\n    groups : int, default 1\n        Number of groups.\n    bias : bool, default False\n        Whether the layer uses a bias vector.\n    use_bn : bool, default True\n        Whether to use BatchNorm layer.\n    bn_eps : float, default 1e-5\n        Small float added to variance in Batch norm.\n    activation : function or str or None, default nn.ReLU(inplace=True)\n        Activation function or name of activation function.\n    \"\"\"\n    return ConvBlock(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=3,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        groups=groups,\n        bias=bias,\n        use_bn=use_bn,\n        bn_eps=bn_eps,\n        activation=activation)\n\n\ndef conv5x5_block(in_channels,\n                  out_channels,\n                  stride=1,\n                  padding=2,\n                  dilation=1,\n                  groups=1,\n                  bias=False,\n                  bn_eps=1e-5,\n                  activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"\n    5x5 version of the standard convolution block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int, default 1\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int, default 2\n        Padding value for convolution layer.\n    dilation : int or tuple/list of 2 int, default 1\n        Dilation value for convolution layer.\n    groups : int, default 1\n        Number of groups.\n    bias : bool, default False\n        Whether the layer uses a bias vector.\n    bn_eps : float, default 1e-5\n        Small float added to variance in Batch norm.\n    activation : function or str or None, default nn.ReLU(inplace=True)\n        Activation function or name of activation function.\n    \"\"\"\n    return ConvBlock(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=5,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        groups=groups,\n        bias=bias,\n        bn_eps=bn_eps,\n        activation=activation)\n\n\ndef conv7x7_block(in_channels,\n                  out_channels,\n                  stride=1,\n                  padding=3,\n                  bias=False,\n                  use_bn=True,\n                  activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"\n    7x7 version of the standard convolution block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int, default 1\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int, default 3\n        Padding value for convolution layer.\n    bias : bool, default False\n        Whether the layer uses a bias vector.\n    use_bn : bool, default True\n        Whether to use BatchNorm layer.\n    activation : function or str or None, default nn.ReLU(inplace=True)\n        Activation function or name of activation function.\n    \"\"\"\n    return ConvBlock(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=7,\n        stride=stride,\n        padding=padding,\n        bias=bias,\n        use_bn=use_bn,\n        activation=activation)\n\n\ndef dwconv_block(in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=1,\n                 dilation=1,\n                 bias=False,\n                 use_bn=True,\n                 bn_eps=1e-5,\n                 activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"\n    Depthwise version of the standard convolution block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    kernel_size : int or tuple/list of 2 int\n        Convolution window size.\n    stride : int or tuple/list of 2 int, default 1\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int, default 1\n        Padding value for convolution layer.\n    dilation : int or tuple/list of 2 int, default 1\n        Dilation value for convolution layer.\n    bias : bool, default False\n        Whether the layer uses a bias vector.\n    use_bn : bool, default True\n        Whether to use BatchNorm layer.\n    bn_eps : float, default 1e-5\n        Small float added to variance in Batch norm.\n    activation : function or str or None, default nn.ReLU(inplace=True)\n        Activation function or name of activation function.\n    \"\"\"\n    return ConvBlock(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        groups=out_channels,\n        bias=bias,\n        use_bn=use_bn,\n        bn_eps=bn_eps,\n        activation=activation)\n\n\ndef dwconv3x3_block(in_channels,\n                    out_channels,\n                    stride=1,\n                    padding=1,\n                    dilation=1,\n                    bias=False,\n                    bn_eps=1e-5,\n                    activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"\n    3x3 depthwise version of the standard convolution block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int, default 1\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int, default 1\n        Padding value for convolution layer.\n    dilation : int or tuple/list of 2 int, default 1\n        Dilation value for convolution layer.\n    bias : bool, default False\n        Whether the layer uses a bias vector.\n    bn_eps : float, default 1e-5\n        Small float added to variance in Batch norm.\n    activation : function or str or None, default nn.ReLU(inplace=True)\n        Activation function or name of activation function.\n    \"\"\"\n    return dwconv_block(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=3,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        bias=bias,\n        bn_eps=bn_eps,\n        activation=activation)\n\n\ndef dwconv5x5_block(in_channels,\n                    out_channels,\n                    stride=1,\n                    padding=2,\n                    dilation=1,\n                    bias=False,\n                    bn_eps=1e-5,\n                    activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"\n    5x5 depthwise version of the standard convolution block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int, default 1\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int, default 2\n        Padding value for convolution layer.\n    dilation : int or tuple/list of 2 int, default 1\n        Dilation value for convolution layer.\n    bias : bool, default False\n        Whether the layer uses a bias vector.\n    bn_eps : float, default 1e-5\n        Small float added to variance in Batch norm.\n    activation : function or str or None, default nn.ReLU(inplace=True)\n        Activation function or name of activation function.\n    \"\"\"\n    return dwconv_block(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=5,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        bias=bias,\n        bn_eps=bn_eps,\n        activation=activation)\n\n\nclass DwsConvBlock(nn.Module):\n    \"\"\"\n    Depthwise separable convolution block with BatchNorms and activations at each convolution layers.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    kernel_size : int or tuple/list of 2 int\n        Convolution window size.\n    stride : int or tuple/list of 2 int\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int\n        Padding value for convolution layer.\n    dilation : int or tuple/list of 2 int, default 1\n        Dilation value for convolution layer.\n    bias : bool, default False\n        Whether the layer uses a bias vector.\n    use_bn : bool, default True\n        Whether to use BatchNorm layer.\n    bn_eps : float, default 1e-5\n        Small float added to variance in Batch norm.\n    dw_activation : function or str or None, default nn.ReLU(inplace=True)\n        Activation function after the depthwise convolution block.\n    pw_activation : function or str or None, default nn.ReLU(inplace=True)\n        Activation function after the pointwise convolution block.\n    \"\"\"\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride,\n                 padding,\n                 dilation=1,\n                 bias=False,\n                 use_bn=True,\n                 bn_eps=1e-5,\n                 dw_activation=(lambda: nn.ReLU(inplace=True)),\n                 pw_activation=(lambda: nn.ReLU(inplace=True))):\n        super(DwsConvBlock, self).__init__()\n        self.dw_conv = dwconv_block(\n            in_channels=in_channels,\n            out_channels=in_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            bias=bias,\n            use_bn=use_bn,\n            bn_eps=bn_eps,\n            activation=dw_activation)\n        self.pw_conv = conv1x1_block(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            bias=bias,\n            use_bn=use_bn,\n            bn_eps=bn_eps,\n            activation=pw_activation)\n\n    def forward(self, x):\n        x = self.dw_conv(x)\n        x = self.pw_conv(x)\n        return x\n\n\ndef dwsconv3x3_block(in_channels,\n                     out_channels,\n                     stride=1,\n                     padding=1,\n                     dilation=1,\n                     bias=False,\n                     bn_eps=1e-5,\n                     dw_activation=(lambda: nn.ReLU(inplace=True)),\n                     pw_activation=(lambda: nn.ReLU(inplace=True))):\n    \"\"\"\n    3x3 depthwise separable version of the standard convolution block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int, default 1\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int, default 1\n        Padding value for convolution layer.\n    dilation : int or tuple/list of 2 int, default 1\n        Dilation value for convolution layer.\n    bias : bool, default False\n        Whether the layer uses a bias vector.\n    bn_eps : float, default 1e-5\n        Small float added to variance in Batch norm.\n    dw_activation : function or str or None, default nn.ReLU(inplace=True)\n        Activation function after the depthwise convolution block.\n    pw_activation : function or str or None, default nn.ReLU(inplace=True)\n        Activation function after the pointwise convolution block.\n    \"\"\"\n    return DwsConvBlock(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=3,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        bias=bias,\n        bn_eps=bn_eps,\n        dw_activation=dw_activation,\n        pw_activation=pw_activation)\n\n\nclass PreConvBlock(nn.Module):\n    \"\"\"\n    Convolution block with Batch normalization and ReLU pre-activation.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    kernel_size : int or tuple/list of 2 int\n        Convolution window size.\n    stride : int or tuple/list of 2 int\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int\n        Padding value for convolution layer.\n    dilation : int or tuple/list of 2 int, default 1\n        Dilation value for convolution layer.\n    bias : bool, default False\n        Whether the layer uses a bias vector.\n    return_preact : bool, default False\n        Whether return pre-activation. It's used by PreResNet.\n    activate : bool, default True\n        Whether activate the convolution block.\n    \"\"\"\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride,\n                 padding,\n                 dilation=1,\n                 bias=False,\n                 return_preact=False,\n                 activate=True):\n        super(PreConvBlock, self).__init__()\n        self.return_preact = return_preact\n        self.activate = activate\n\n        self.bn = nn.BatchNorm2d(num_features=in_channels)\n        if self.activate:\n            self.activ = nn.ReLU(inplace=True)\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            bias=bias)\n\n    def forward(self, x):\n        x = self.bn(x)\n        if self.activate:\n            x = self.activ(x)\n        if self.return_preact:\n            x_pre_activ = x\n        x = self.conv(x)\n        if self.return_preact:\n            return x, x_pre_activ\n        else:\n            return x\n\n\ndef pre_conv1x1_block(in_channels,\n                      out_channels,\n                      stride=1,\n                      bias=False,\n                      return_preact=False,\n                      activate=True):\n    \"\"\"\n    1x1 version of the pre-activated convolution block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int, default 1\n        Strides of the convolution.\n    bias : bool, default False\n        Whether the layer uses a bias vector.\n    return_preact : bool, default False\n        Whether return pre-activation.\n    activate : bool, default True\n        Whether activate the convolution block.\n    \"\"\"\n    return PreConvBlock(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=1,\n        stride=stride,\n        padding=0,\n        bias=bias,\n        return_preact=return_preact,\n        activate=activate)\n\n\ndef pre_conv3x3_block(in_channels,\n                      out_channels,\n                      stride=1,\n                      padding=1,\n                      dilation=1,\n                      return_preact=False,\n                      activate=True):\n    \"\"\"\n    3x3 version of the pre-activated convolution block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int, default 1\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int, default 1\n        Padding value for convolution layer.\n    dilation : int or tuple/list of 2 int, default 1\n        Dilation value for convolution layer.\n    return_preact : bool, default False\n        Whether return pre-activation.\n    activate : bool, default True\n        Whether activate the convolution block.\n    \"\"\"\n    return PreConvBlock(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=3,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        return_preact=return_preact,\n        activate=activate)\n\n\ndef channel_shuffle(x,\n                    groups):\n    \"\"\"\n    Channel shuffle operation from 'ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,'\n    https://arxiv.org/abs/1707.01083.\n\n    Parameters:\n    ----------\n    x : Tensor\n        Input tensor.\n    groups : int\n        Number of groups.\n\n    Returns\n    -------\n    Tensor\n        Resulted tensor.\n    \"\"\"\n    batch, channels, height, width = x.size()\n    # assert (channels % groups == 0)\n    channels_per_group = channels // groups\n    x = x.view(batch, groups, channels_per_group, height, width)\n    x = torch.transpose(x, 1, 2).contiguous()\n    x = x.view(batch, channels, height, width)\n    return x\n\n\nclass ChannelShuffle(nn.Module):\n    \"\"\"\n    Channel shuffle layer. This is a wrapper over the same operation. It is designed to save the number of groups.\n\n    Parameters:\n    ----------\n    channels : int\n        Number of channels.\n    groups : int\n        Number of groups.\n    \"\"\"\n    def __init__(self,\n                 channels,\n                 groups):\n        super(ChannelShuffle, self).__init__()\n        # assert (channels % groups == 0)\n        if channels % groups != 0:\n            raise ValueError('channels must be divisible by groups')\n        self.groups = groups\n\n    def forward(self, x):\n        return channel_shuffle(x, self.groups)\n\n\ndef channel_shuffle2(x,\n                     groups):\n    \"\"\"\n    Channel shuffle operation from 'ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,'\n    https://arxiv.org/abs/1707.01083. The alternative version.\n\n    Parameters:\n    ----------\n    x : Tensor\n        Input tensor.\n    groups : int\n        Number of groups.\n\n    Returns\n    -------\n    Tensor\n        Resulted tensor.\n    \"\"\"\n    batch, channels, height, width = x.size()\n    # assert (channels % groups == 0)\n    channels_per_group = channels // groups\n    x = x.view(batch, channels_per_group, groups, height, width)\n    x = torch.transpose(x, 1, 2).contiguous()\n    x = x.view(batch, channels, height, width)\n    return x\n\n\nclass ChannelShuffle2(nn.Module):\n    \"\"\"\n    Channel shuffle layer. This is a wrapper over the same operation. It is designed to save the number of groups.\n    The alternative version.\n\n    Parameters:\n    ----------\n    channels : int\n        Number of channels.\n    groups : int\n        Number of groups.\n    \"\"\"\n    def __init__(self,\n                 channels,\n                 groups):\n        super(ChannelShuffle2, self).__init__()\n        # assert (channels % groups == 0)\n        if channels % groups != 0:\n            raise ValueError('channels must be divisible by groups')\n        self.groups = groups\n\n    def forward(self, x):\n        return channel_shuffle2(x, self.groups)\n\n\nclass SEBlock(nn.Module):\n    \"\"\"\n    Squeeze-and-Excitation block from 'Squeeze-and-Excitation Networks,' https://arxiv.org/abs/1709.01507.\n\n    Parameters:\n    ----------\n    channels : int\n        Number of channels.\n    reduction : int, default 16\n        Squeeze reduction value.\n    round_mid : bool, default False\n        Whether to round middle channel number (make divisible by 8).\n    activation : function, or str, or nn.Module, default 'relu'\n        Activation function after the first convolution.\n    out_activation : function, or str, or nn.Module, default 'sigmoid'\n        Activation function after the last convolution.\n    \"\"\"\n    def __init__(self,\n                 channels,\n                 reduction=16,\n                 round_mid=False,\n                 mid_activation=(lambda: nn.ReLU(inplace=True)),\n                 out_activation=(lambda: nn.Sigmoid())):\n        super(SEBlock, self).__init__()\n        mid_channels = channels // reduction if not round_mid else round_channels(float(channels) / reduction)\n\n        self.pool = nn.AdaptiveAvgPool2d(output_size=1)\n        self.conv1 = conv1x1(\n            in_channels=channels,\n            out_channels=mid_channels,\n            bias=True)\n        self.activ = get_activation_layer(mid_activation)\n        self.conv2 = conv1x1(\n            in_channels=mid_channels,\n            out_channels=channels,\n            bias=True)\n        self.sigmoid = get_activation_layer(out_activation)\n\n    def forward(self, x):\n        w = self.pool(x)\n        w = self.conv1(w)\n        w = self.activ(w)\n        w = self.conv2(w)\n        w = self.sigmoid(w)\n        x = x * w\n        return x\n\n\nclass IBN(nn.Module):\n    \"\"\"\n    Instance-Batch Normalization block from 'Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net,'\n    https://arxiv.org/abs/1807.09441.\n\n    Parameters:\n    ----------\n    channels : int\n        Number of channels.\n    inst_fraction : float, default 0.5\n        The first fraction of channels for normalization.\n    inst_first : bool, default True\n        Whether instance normalization be on the first part of channels.\n    \"\"\"\n    def __init__(self,\n                 channels,\n                 first_fraction=0.5,\n                 inst_first=True):\n        super(IBN, self).__init__()\n        self.inst_first = inst_first\n        h1_channels = int(math.floor(channels * first_fraction))\n        h2_channels = channels - h1_channels\n        self.split_sections = [h1_channels, h2_channels]\n\n        if self.inst_first:\n            self.inst_norm = nn.InstanceNorm2d(\n                num_features=h1_channels,\n                affine=True)\n            self.batch_norm = nn.BatchNorm2d(num_features=h2_channels)\n        else:\n            self.batch_norm = nn.BatchNorm2d(num_features=h1_channels)\n            self.inst_norm = nn.InstanceNorm2d(\n                num_features=h2_channels,\n                affine=True)\n\n    def forward(self, x):\n        x1, x2 = torch.split(x, split_size_or_sections=self.split_sections, dim=1)\n        if self.inst_first:\n            x1 = self.inst_norm(x1.contiguous())\n            x2 = self.batch_norm(x2.contiguous())\n        else:\n            x1 = self.batch_norm(x1.contiguous())\n            x2 = self.inst_norm(x2.contiguous())\n        x = torch.cat((x1, x2), dim=1)\n        return x\n\n\nclass Identity(nn.Module):\n    \"\"\"\n    Identity block.\n    \"\"\"\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def forward(self, x):\n        return x\n\n\nclass DualPathSequential(nn.Sequential):\n    \"\"\"\n    A sequential container for modules with dual inputs/outputs.\n    Modules will be executed in the order they are added.\n\n    Parameters:\n    ----------\n    return_two : bool, default True\n        Whether to return two output after execution.\n    first_ordinals : int, default 0\n        Number of the first modules with single input/output.\n    last_ordinals : int, default 0\n        Number of the final modules with single input/output.\n    dual_path_scheme : function\n        Scheme of dual path response for a module.\n    dual_path_scheme_ordinal : function\n        Scheme of dual path response for an ordinal module.\n    \"\"\"\n    def __init__(self,\n                 return_two=True,\n                 first_ordinals=0,\n                 last_ordinals=0,\n                 dual_path_scheme=(lambda module, x1, x2: module(x1, x2)),\n                 dual_path_scheme_ordinal=(lambda module, x1, x2: (module(x1), x2))):\n        super(DualPathSequential, self).__init__()\n        self.return_two = return_two\n        self.first_ordinals = first_ordinals\n        self.last_ordinals = last_ordinals\n        self.dual_path_scheme = dual_path_scheme\n        self.dual_path_scheme_ordinal = dual_path_scheme_ordinal\n\n    def forward(self, x1, x2=None):\n        length = len(self._modules.values())\n        for i, module in enumerate(self._modules.values()):\n            if (i < self.first_ordinals) or (i >= length - self.last_ordinals):\n                x1, x2 = self.dual_path_scheme_ordinal(module, x1, x2)\n            else:\n                x1, x2 = self.dual_path_scheme(module, x1, x2)\n        if self.return_two:\n            return x1, x2\n        else:\n            return x1\n\n\nclass Concurrent(nn.Sequential):\n    \"\"\"\n    A container for concatenation of modules on the base of the sequential container.\n\n    Parameters:\n    ----------\n    axis : int, default 1\n        The axis on which to concatenate the outputs.\n    stack : bool, default False\n        Whether to concatenate tensors along a new dimension.\n    \"\"\"\n    def __init__(self,\n                 axis=1,\n                 stack=False):\n        super(Concurrent, self).__init__()\n        self.axis = axis\n        self.stack = stack\n\n    def forward(self, x):\n        out = []\n        for module in self._modules.values():\n            out.append(module(x))\n        if self.stack:\n            out = torch.stack(tuple(out), dim=self.axis)\n        else:\n            out = torch.cat(tuple(out), dim=self.axis)\n        return out\n\n\nclass ParametricSequential(nn.Sequential):\n    \"\"\"\n    A sequential container for modules with parameters.\n    Modules will be executed in the order they are added.\n    \"\"\"\n    def __init__(self, *args):\n        super(ParametricSequential, self).__init__(*args)\n\n    def forward(self, x, **kwargs):\n        for module in self._modules.values():\n            x = module(x, **kwargs)\n        return x\n\n\nclass ParametricConcurrent(nn.Sequential):\n    \"\"\"\n    A container for concatenation of modules with parameters.\n\n    Parameters:\n    ----------\n    axis : int, default 1\n        The axis on which to concatenate the outputs.\n    \"\"\"\n    def __init__(self, axis=1):\n        super(ParametricConcurrent, self).__init__()\n        self.axis = axis\n\n    def forward(self, x, **kwargs):\n        out = []\n        for module in self._modules.values():\n            out.append(module(x, **kwargs))\n        out = torch.cat(tuple(out), dim=self.axis)\n        return out\n\n\nclass Hourglass(nn.Module):\n    \"\"\"\n    A hourglass block.\n\n    Parameters:\n    ----------\n    down_seq : nn.Sequential\n        Down modules as sequential.\n    up_seq : nn.Sequential\n        Up modules as sequential.\n    skip_seq : nn.Sequential\n        Skip connection modules as sequential.\n    merge_type : str, default 'add'\n        Type of concatenation of up and skip outputs.\n    return_first_skip : bool, default False\n        Whether return the first skip connection output. Used in ResAttNet.\n    \"\"\"\n    def __init__(self,\n                 down_seq,\n                 up_seq,\n                 skip_seq,\n                 merge_type=\"add\",\n                 return_first_skip=False):\n        super(Hourglass, self).__init__()\n        assert (len(up_seq) == len(down_seq))\n        assert (len(skip_seq) == len(down_seq))\n        assert (merge_type in [\"add\"])\n        self.merge_type = merge_type\n        self.return_first_skip = return_first_skip\n        self.depth = len(down_seq)\n\n        self.down_seq = down_seq\n        self.up_seq = up_seq\n        self.skip_seq = skip_seq\n\n    def forward(self, x, **kwargs):\n        y = None\n        down_outs = [x]\n        for down_module in self.down_seq._modules.values():\n            x = down_module(x)\n            down_outs.append(x)\n        for i in range(len(down_outs)):\n            if i != 0:\n                y = down_outs[self.depth - i]\n                skip_module = self.skip_seq[self.depth - i]\n                y = skip_module(y)\n                if (y is not None) and (self.merge_type == \"add\"):\n                    x = x + y\n            if i != len(down_outs) - 1:\n                up_module = self.up_seq[self.depth - 1 - i]\n                x = up_module(x)\n        if self.return_first_skip:\n            return x, y\n        else:\n            return x\n\n\nclass SesquialteralHourglass(nn.Module):\n    \"\"\"\n    A sesquialteral hourglass block.\n\n    Parameters:\n    ----------\n    down1_seq : nn.Sequential\n        The first down modules as sequential.\n    skip1_seq : nn.Sequential\n        The first skip connection modules as sequential.\n    up_seq : nn.Sequential\n        Up modules as sequential.\n    skip2_seq : nn.Sequential\n        The second skip connection modules as sequential.\n    down2_seq : nn.Sequential\n        The second down modules as sequential.\n    merge_type : str, default 'con'\n        Type of concatenation of up and skip outputs.\n    \"\"\"\n    def __init__(self,\n                 down1_seq,\n                 skip1_seq,\n                 up_seq,\n                 skip2_seq,\n                 down2_seq,\n                 merge_type=\"cat\"):\n        super(SesquialteralHourglass, self).__init__()\n        assert (len(down1_seq) == len(up_seq))\n        assert (len(down1_seq) == len(down2_seq))\n        assert (len(skip1_seq) == len(skip2_seq))\n        assert (len(down1_seq) == len(skip1_seq) - 1)\n        assert (merge_type in [\"cat\", \"add\"])\n        self.merge_type = merge_type\n        self.depth = len(down1_seq)\n\n        self.down1_seq = down1_seq\n        self.skip1_seq = skip1_seq\n        self.up_seq = up_seq\n        self.skip2_seq = skip2_seq\n        self.down2_seq = down2_seq\n\n    def _merge(self, x, y):\n        if y is not None:\n            if self.merge_type == \"cat\":\n                x = torch.cat((x, y), dim=1)\n            elif self.merge_type == \"add\":\n                x = x + y\n        return x\n\n    def forward(self, x, **kwargs):\n        y = self.skip1_seq[0](x)\n        skip1_outs = [y]\n        for i in range(self.depth):\n            x = self.down1_seq[i](x)\n            y = self.skip1_seq[i + 1](x)\n            skip1_outs.append(y)\n        x = skip1_outs[self.depth]\n        y = self.skip2_seq[0](x)\n        skip2_outs = [y]\n        for i in range(self.depth):\n            x = self.up_seq[i](x)\n            y = skip1_outs[self.depth - 1 - i]\n            x = self._merge(x, y)\n            y = self.skip2_seq[i + 1](x)\n            skip2_outs.append(y)\n        x = self.skip2_seq[self.depth](x)\n        for i in range(self.depth):\n            x = self.down2_seq[i](x)\n            y = skip2_outs[self.depth - 1 - i]\n            x = self._merge(x, y)\n        return x\n\n\nclass MultiOutputSequential(nn.Sequential):\n    \"\"\"\n    A sequential container with multiple outputs.\n    Modules will be executed in the order they are added.\n    \"\"\"\n    def __init__(self):\n        super(MultiOutputSequential, self).__init__()\n\n    def forward(self, x):\n        outs = []\n        for module in self._modules.values():\n            x = module(x)\n            if hasattr(module, \"do_output\") and module.do_output:\n                outs.append(x)\n        return [x] + outs\n\n\nclass Flatten(nn.Module):\n    \"\"\"\n    Simple flatten module.\n    \"\"\"\n\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n# SqueezeNet","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-02-04T04:21:14.183730Z","iopub.execute_input":"2025-02-04T04:21:14.184146Z","iopub.status.idle":"2025-02-04T04:21:14.245139Z","shell.execute_reply.started":"2025-02-04T04:21:14.184111Z","shell.execute_reply":"2025-02-04T04:21:14.244197Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Train Model Function","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=CFG.EPOCHS, patience=7, min_lr=1e-6):\n    best_accuracy = 0.0\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Initialize learning rate scheduler without verbose parameter\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.1, patience=3, min_lr=min_lr\n    )\n    \n    # Early stopping parameters\n    early_stopping_patience = patience\n    early_stopping_counter = 0\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        train_loss = 0.0\n        train_preds = []\n        train_labels = []\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            train_preds.extend(preds.cpu().numpy())\n            train_labels.extend(labels.cpu().numpy())\n            \n        # Calculate training metrics\n        train_accuracy = accuracy_score(train_labels, train_preds)\n        train_f1 = f1_score(train_labels, train_preds, average='weighted')\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_preds = []\n        val_labels = []\n        \n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, preds = torch.max(outputs, 1)\n                val_preds.extend(preds.cpu().numpy())\n                val_labels.extend(labels.cpu().numpy())\n                \n        # Calculate validation metrics\n        val_accuracy = accuracy_score(val_labels, val_preds)\n        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n        \n        # Print metrics\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_accuracy:.4f}, Train F1: {train_f1:.4f}\")\n        print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n        \n        # Print confusion matrix every 5 epochs\n        # if epoch % 5 == 0:\n        #     cm = confusion_matrix(val_labels, val_preds)\n        \n        # Learning rate scheduling\n        old_lr = optimizer.param_groups[0]['lr']\n        scheduler.step(val_accuracy)\n        new_lr = optimizer.param_groups[0]['lr']\n        \n        # Only print LR changes when they occur\n        if old_lr != new_lr:\n            print(f\"Learning rate decreased from {old_lr:.6f} to {new_lr:.6f}\")\n        \n        # Save the best model and update early stopping\n        if val_accuracy > best_accuracy:\n            best_accuracy = val_accuracy\n            torch.save(model.state_dict(), 'best_model.pth')\n            early_stopping_counter = 0\n        else:\n            early_stopping_counter += 1\n            \n        # Early stopping check\n        # if early_stopping_counter >= early_stopping_patience:\n        #     print(f\"\\nEarly stopping triggered after {epoch+1} epochs!\")\n        #     break\n            \n        # If learning rate is too small, stop training\n        if new_lr <= min_lr:\n            print(f\"\\nLearning rate {new_lr:.6f} is too small. Stopping training!\")\n            break\n    \n    print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n    return best_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:21:14.614328Z","iopub.execute_input":"2025-02-04T04:21:14.614639Z","iopub.status.idle":"2025-02-04T04:21:14.624497Z","shell.execute_reply.started":"2025-02-04T04:21:14.614616Z","shell.execute_reply":"2025-02-04T04:21:14.623496Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Experiment 1 [EfficientNet + SqueezeNet + CBAM + SE Block]","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        \n        self.fc = nn.Sequential(\n            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n        )\n        \n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return torch.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        \n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv(x)\n        return torch.sigmoid(x)\n\nclass CBAM(nn.Module):\n    def __init__(self, channels, reduction=16, kernel_size=7):\n        super(CBAM, self).__init__()\n        self.channel_att = ChannelAttention(channels, reduction)\n        self.spatial_att = SpatialAttention(kernel_size)\n        \n    def forward(self, x):\n        x = x * self.channel_att(x)\n        x = x * self.spatial_att(x)\n        return x\n\nclass HybridFireUnit(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand_channels, residual=True):\n        super(HybridFireUnit, self).__init__()\n        self.residual = residual and (in_channels == expand_channels * 2)\n        \n        self.squeeze = conv1x1_block(\n            in_channels=in_channels,\n            out_channels=squeeze_channels,\n            activation=\"relu\")\n            \n        self.expand1x1 = conv1x1_block(\n            in_channels=squeeze_channels,\n            out_channels=expand_channels,\n            activation=\"relu\")\n            \n        self.expand3x3 = conv3x3_block(\n            in_channels=squeeze_channels,\n            out_channels=expand_channels,\n            activation=\"relu\")\n            \n        self.cbam = CBAM(expand_channels * 2)\n        \n    def forward(self, x):\n        identity = x\n        \n        x = self.squeeze(x)\n        e1 = self.expand1x1(x)\n        e3 = self.expand3x3(x)\n        out = torch.cat([e1, e3], dim=1)\n        \n        out = self.cbam(out)\n        \n        if self.residual:\n            out = out + identity\n        return out\n\nclass EfficientFireBlock(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand_channels, stride=1):\n        super(EfficientFireBlock, self).__init__()\n        self.conv1 = conv1x1_block(\n            in_channels=in_channels,\n            out_channels=squeeze_channels,\n            activation=\"swish\")\n            \n        self.fire = HybridFireUnit(\n            in_channels=squeeze_channels,\n            squeeze_channels=squeeze_channels // 4,\n            expand_channels=expand_channels // 2)\n            \n        self.se = SEBlock(\n            channels=expand_channels,\n            reduction=4,\n            mid_activation=\"swish\")\n            \n        if stride > 1:\n            self.pool = nn.MaxPool2d(kernel_size=stride, stride=stride)\n        else:\n            self.pool = None\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.fire(x)\n        x = self.se(x)\n        if self.pool is not None:\n            x = self.pool(x)\n        return x\n\nclass HybridNet(nn.Module):\n    def __init__(self,\n                 init_channels=32,\n                 channels=[64, 128, 256, 512],\n                 scale_factor=1.0,\n                 num_classes=1000):\n        super(HybridNet, self).__init__()\n        \n        channels = [int(c * scale_factor) for c in channels]\n        \n        self.init_block = conv3x3_block(\n            in_channels=3,\n            out_channels=init_channels,\n            stride=2,\n            activation=\"swish\")\n            \n        self.stages = nn.ModuleList()\n        in_channels = init_channels\n        \n        for i, out_channels in enumerate(channels):\n            stage = nn.Sequential(\n                EfficientFireBlock(\n                    in_channels=in_channels,\n                    squeeze_channels=out_channels // 2,\n                    expand_channels=out_channels,\n                    stride=2 if i > 0 else 1),\n                EfficientFireBlock(\n                    in_channels=out_channels,\n                    squeeze_channels=out_channels // 2,\n                    expand_channels=out_channels)\n            )\n            self.stages.append(stage)\n            in_channels = out_channels\n            \n\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.dropout = nn.Dropout(0.2)\n        self.fc = nn.Linear(channels[-1], num_classes)\n        \n        self._initialize_weights()\n        \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n                \n    def forward(self, x):\n        x = self.init_block(x)\n        \n        for stage in self.stages:\n            x = stage(x)\n            \n        x = self.global_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x\n\ndef create_model_1(num_classes=1000, scale=1.0):\n    return HybridNet(\n        init_channels=32,\n        scale_factor=scale,\n        num_classes=num_classes\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T05:19:43.138661Z","iopub.execute_input":"2025-01-30T05:19:43.138998Z","iopub.status.idle":"2025-01-30T05:19:43.156467Z","shell.execute_reply.started":"2025-01-30T05:19:43.138971Z","shell.execute_reply":"2025-01-30T05:19:43.155708Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Initialize model\nmodel = create_model_1(num_classes=2, scale = 1.0)  # Set to 2 classes\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(num_params)\n\n# Transfer to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Setup optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.1, patience=3\n)\n\n# Calculate class weights\nclass_counts = train_new_df['label_encoded'].value_counts()\nclass_weights = torch.FloatTensor([1/class_counts[0], 1/class_counts[1]]).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Train the model\ntrain_model(\n    model=model, \n    train_loader=train_loader, \n    val_loader=val_loader, \n    criterion=criterion, \n    optimizer=optimizer, \n    num_epochs=100\n)\n\n# Load the best model\nmodel.load_state_dict(torch.load('best_model.pth', weights_only = True))\nmodel.eval()\n\n# Evaluate on test set\ncorrect_test = 0\ntotal_test = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        correct_test += torch.sum(preds == labels)\n        total_test += labels.size(0)\n\ntest_accuracy = correct_test.double() / total_test\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T06:08:09.763976Z","iopub.execute_input":"2025-01-30T06:08:09.764320Z","iopub.status.idle":"2025-01-30T08:35:59.181070Z","shell.execute_reply.started":"2025-01-30T06:08:09.764294Z","shell.execute_reply":"2025-01-30T08:35:59.180010Z"}},"outputs":[{"name":"stdout","text":"1186418\nEpoch 1/100\nTrain Loss: 0.6590, Train Acc: 0.5921, Train F1: 0.5924\nVal Loss: 0.4703, Val Acc: 0.7677, Val F1: 0.7679\nEpoch 2/100\nTrain Loss: 0.5456, Train Acc: 0.7091, Train F1: 0.7071\nVal Loss: 0.4220, Val Acc: 0.7778, Val F1: 0.7768\nEpoch 3/100\nTrain Loss: 0.5165, Train Acc: 0.7325, Train F1: 0.7315\nVal Loss: 0.4004, Val Acc: 0.8081, Val F1: 0.8085\nEpoch 4/100\nTrain Loss: 0.5005, Train Acc: 0.7457, Train F1: 0.7451\nVal Loss: 0.3786, Val Acc: 0.8182, Val F1: 0.8183\nEpoch 5/100\nTrain Loss: 0.4791, Train Acc: 0.7562, Train F1: 0.7560\nVal Loss: 0.3865, Val Acc: 0.8131, Val F1: 0.8135\nEpoch 6/100\nTrain Loss: 0.4666, Train Acc: 0.7635, Train F1: 0.7632\nVal Loss: 0.3618, Val Acc: 0.8207, Val F1: 0.8209\nEpoch 7/100\nTrain Loss: 0.4534, Train Acc: 0.7770, Train F1: 0.7770\nVal Loss: 0.3427, Val Acc: 0.8460, Val F1: 0.8462\nEpoch 8/100\nTrain Loss: 0.4457, Train Acc: 0.7803, Train F1: 0.7804\nVal Loss: 0.3384, Val Acc: 0.8359, Val F1: 0.8357\nEpoch 9/100\nTrain Loss: 0.4326, Train Acc: 0.7882, Train F1: 0.7883\nVal Loss: 0.3219, Val Acc: 0.8510, Val F1: 0.8512\nEpoch 10/100\nTrain Loss: 0.4205, Train Acc: 0.7920, Train F1: 0.7920\nVal Loss: 0.3555, Val Acc: 0.8232, Val F1: 0.8236\nEpoch 11/100\nTrain Loss: 0.4085, Train Acc: 0.8007, Train F1: 0.8008\nVal Loss: 0.3166, Val Acc: 0.8485, Val F1: 0.8488\nEpoch 12/100\nTrain Loss: 0.4032, Train Acc: 0.8061, Train F1: 0.8063\nVal Loss: 0.2895, Val Acc: 0.8712, Val F1: 0.8715\nEpoch 13/100\nTrain Loss: 0.4003, Train Acc: 0.8057, Train F1: 0.8060\nVal Loss: 0.2916, Val Acc: 0.8687, Val F1: 0.8689\nEpoch 14/100\nTrain Loss: 0.3921, Train Acc: 0.8140, Train F1: 0.8143\nVal Loss: 0.2864, Val Acc: 0.8662, Val F1: 0.8664\nEpoch 15/100\nTrain Loss: 0.3818, Train Acc: 0.8208, Train F1: 0.8210\nVal Loss: 0.2865, Val Acc: 0.8788, Val F1: 0.8790\nEpoch 16/100\nTrain Loss: 0.3776, Train Acc: 0.8217, Train F1: 0.8220\nVal Loss: 0.2879, Val Acc: 0.8712, Val F1: 0.8714\nEpoch 17/100\nTrain Loss: 0.3662, Train Acc: 0.8253, Train F1: 0.8255\nVal Loss: 0.2903, Val Acc: 0.8788, Val F1: 0.8790\nEpoch 18/100\nTrain Loss: 0.3657, Train Acc: 0.8259, Train F1: 0.8262\nVal Loss: 0.2670, Val Acc: 0.8990, Val F1: 0.8992\nEpoch 19/100\nTrain Loss: 0.3565, Train Acc: 0.8295, Train F1: 0.8298\nVal Loss: 0.2694, Val Acc: 0.8965, Val F1: 0.8967\nEpoch 20/100\nTrain Loss: 0.3611, Train Acc: 0.8291, Train F1: 0.8294\nVal Loss: 0.2839, Val Acc: 0.8510, Val F1: 0.8513\nEpoch 21/100\nTrain Loss: 0.3505, Train Acc: 0.8334, Train F1: 0.8337\nVal Loss: 0.2847, Val Acc: 0.8763, Val F1: 0.8763\nEpoch 22/100\nTrain Loss: 0.3483, Train Acc: 0.8371, Train F1: 0.8374\nVal Loss: 0.2653, Val Acc: 0.8788, Val F1: 0.8790\nLearning rate decreased from 0.000100 to 0.000010\nEpoch 23/100\nTrain Loss: 0.3292, Train Acc: 0.8457, Train F1: 0.8460\nVal Loss: 0.2458, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 24/100\nTrain Loss: 0.3218, Train Acc: 0.8493, Train F1: 0.8496\nVal Loss: 0.2520, Val Acc: 0.8889, Val F1: 0.8891\nEpoch 25/100\nTrain Loss: 0.3271, Train Acc: 0.8480, Train F1: 0.8483\nVal Loss: 0.2442, Val Acc: 0.8864, Val F1: 0.8866\nEpoch 26/100\nTrain Loss: 0.3209, Train Acc: 0.8474, Train F1: 0.8477\nVal Loss: 0.2509, Val Acc: 0.8788, Val F1: 0.8790\nLearning rate decreased from 0.000010 to 0.000001\nEpoch 27/100\nTrain Loss: 0.3225, Train Acc: 0.8487, Train F1: 0.8490\nVal Loss: 0.2554, Val Acc: 0.8813, Val F1: 0.8815\nEpoch 28/100\nTrain Loss: 0.3190, Train Acc: 0.8502, Train F1: 0.8505\nVal Loss: 0.2467, Val Acc: 0.8889, Val F1: 0.8890\nEpoch 29/100\nTrain Loss: 0.3160, Train Acc: 0.8518, Train F1: 0.8521\nVal Loss: 0.2506, Val Acc: 0.8889, Val F1: 0.8890\nEpoch 30/100\nTrain Loss: 0.3194, Train Acc: 0.8541, Train F1: 0.8544\nVal Loss: 0.2433, Val Acc: 0.8864, Val F1: 0.8865\nEpoch 31/100\nTrain Loss: 0.3192, Train Acc: 0.8487, Train F1: 0.8490\nVal Loss: 0.2436, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 32/100\nTrain Loss: 0.3169, Train Acc: 0.8503, Train F1: 0.8506\nVal Loss: 0.2399, Val Acc: 0.8889, Val F1: 0.8891\nEpoch 33/100\nTrain Loss: 0.3157, Train Acc: 0.8517, Train F1: 0.8520\nVal Loss: 0.2492, Val Acc: 0.8838, Val F1: 0.8839\nEpoch 34/100\nTrain Loss: 0.3215, Train Acc: 0.8477, Train F1: 0.8481\nVal Loss: 0.2411, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 35/100\nTrain Loss: 0.3166, Train Acc: 0.8487, Train F1: 0.8490\nVal Loss: 0.2416, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 36/100\nTrain Loss: 0.3167, Train Acc: 0.8506, Train F1: 0.8509\nVal Loss: 0.2474, Val Acc: 0.8864, Val F1: 0.8865\nEpoch 37/100\nTrain Loss: 0.3182, Train Acc: 0.8521, Train F1: 0.8524\nVal Loss: 0.2483, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 38/100\nTrain Loss: 0.3163, Train Acc: 0.8537, Train F1: 0.8540\nVal Loss: 0.2459, Val Acc: 0.8864, Val F1: 0.8865\nEpoch 39/100\nTrain Loss: 0.3177, Train Acc: 0.8495, Train F1: 0.8498\nVal Loss: 0.2414, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 40/100\nTrain Loss: 0.3158, Train Acc: 0.8498, Train F1: 0.8501\nVal Loss: 0.2454, Val Acc: 0.8838, Val F1: 0.8840\nEpoch 41/100\nTrain Loss: 0.3166, Train Acc: 0.8535, Train F1: 0.8538\nVal Loss: 0.2464, Val Acc: 0.8889, Val F1: 0.8890\nEpoch 42/100\nTrain Loss: 0.3182, Train Acc: 0.8517, Train F1: 0.8520\nVal Loss: 0.2551, Val Acc: 0.8813, Val F1: 0.8814\nEpoch 43/100\nTrain Loss: 0.3167, Train Acc: 0.8527, Train F1: 0.8530\nVal Loss: 0.2367, Val Acc: 0.8939, Val F1: 0.8942\nEpoch 44/100\nTrain Loss: 0.3145, Train Acc: 0.8536, Train F1: 0.8539\nVal Loss: 0.2408, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 45/100\nTrain Loss: 0.3142, Train Acc: 0.8520, Train F1: 0.8523\nVal Loss: 0.2493, Val Acc: 0.8889, Val F1: 0.8890\nEpoch 46/100\nTrain Loss: 0.3112, Train Acc: 0.8528, Train F1: 0.8531\nVal Loss: 0.2508, Val Acc: 0.8889, Val F1: 0.8891\nEpoch 47/100\nTrain Loss: 0.3095, Train Acc: 0.8539, Train F1: 0.8542\nVal Loss: 0.2492, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 48/100\nTrain Loss: 0.3179, Train Acc: 0.8527, Train F1: 0.8531\nVal Loss: 0.2467, Val Acc: 0.8889, Val F1: 0.8891\nEpoch 49/100\nTrain Loss: 0.3112, Train Acc: 0.8525, Train F1: 0.8528\nVal Loss: 0.2448, Val Acc: 0.8864, Val F1: 0.8865\nEpoch 50/100\nTrain Loss: 0.3147, Train Acc: 0.8509, Train F1: 0.8512\nVal Loss: 0.2445, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 51/100\nTrain Loss: 0.3126, Train Acc: 0.8540, Train F1: 0.8543\nVal Loss: 0.2439, Val Acc: 0.8889, Val F1: 0.8890\nEpoch 52/100\nTrain Loss: 0.3123, Train Acc: 0.8536, Train F1: 0.8539\nVal Loss: 0.2367, Val Acc: 0.8889, Val F1: 0.8891\nEpoch 53/100\nTrain Loss: 0.3161, Train Acc: 0.8481, Train F1: 0.8484\nVal Loss: 0.2463, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 54/100\nTrain Loss: 0.3084, Train Acc: 0.8544, Train F1: 0.8547\nVal Loss: 0.2517, Val Acc: 0.8864, Val F1: 0.8865\nEpoch 55/100\nTrain Loss: 0.3107, Train Acc: 0.8544, Train F1: 0.8547\nVal Loss: 0.2390, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 56/100\nTrain Loss: 0.3109, Train Acc: 0.8552, Train F1: 0.8555\nVal Loss: 0.2465, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 57/100\nTrain Loss: 0.3135, Train Acc: 0.8544, Train F1: 0.8546\nVal Loss: 0.2523, Val Acc: 0.8864, Val F1: 0.8865\nEpoch 58/100\nTrain Loss: 0.3145, Train Acc: 0.8511, Train F1: 0.8515\nVal Loss: 0.2442, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 59/100\nTrain Loss: 0.3155, Train Acc: 0.8477, Train F1: 0.8481\nVal Loss: 0.2431, Val Acc: 0.8889, Val F1: 0.8890\nEpoch 60/100\nTrain Loss: 0.3174, Train Acc: 0.8533, Train F1: 0.8536\nVal Loss: 0.2405, Val Acc: 0.8965, Val F1: 0.8967\nEpoch 61/100\nTrain Loss: 0.3091, Train Acc: 0.8510, Train F1: 0.8514\nVal Loss: 0.2445, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 62/100\nTrain Loss: 0.3133, Train Acc: 0.8559, Train F1: 0.8562\nVal Loss: 0.2469, Val Acc: 0.8889, Val F1: 0.8891\nEpoch 63/100\nTrain Loss: 0.3158, Train Acc: 0.8541, Train F1: 0.8544\nVal Loss: 0.2572, Val Acc: 0.8889, Val F1: 0.8890\nEpoch 64/100\nTrain Loss: 0.3147, Train Acc: 0.8527, Train F1: 0.8531\nVal Loss: 0.2476, Val Acc: 0.8864, Val F1: 0.8865\nEpoch 65/100\nTrain Loss: 0.3065, Train Acc: 0.8568, Train F1: 0.8572\nVal Loss: 0.2380, Val Acc: 0.8939, Val F1: 0.8942\nEpoch 66/100\nTrain Loss: 0.3117, Train Acc: 0.8560, Train F1: 0.8563\nVal Loss: 0.2408, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 67/100\nTrain Loss: 0.3108, Train Acc: 0.8544, Train F1: 0.8547\nVal Loss: 0.2431, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 68/100\nTrain Loss: 0.3093, Train Acc: 0.8572, Train F1: 0.8575\nVal Loss: 0.2551, Val Acc: 0.8914, Val F1: 0.8915\nEpoch 69/100\nTrain Loss: 0.3170, Train Acc: 0.8526, Train F1: 0.8529\nVal Loss: 0.2449, Val Acc: 0.8965, Val F1: 0.8967\nEpoch 70/100\nTrain Loss: 0.3102, Train Acc: 0.8525, Train F1: 0.8528\nVal Loss: 0.2463, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 71/100\nTrain Loss: 0.3075, Train Acc: 0.8558, Train F1: 0.8561\nVal Loss: 0.2382, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 72/100\nTrain Loss: 0.3148, Train Acc: 0.8528, Train F1: 0.8531\nVal Loss: 0.2423, Val Acc: 0.8838, Val F1: 0.8840\nEpoch 73/100\nTrain Loss: 0.3054, Train Acc: 0.8580, Train F1: 0.8583\nVal Loss: 0.2392, Val Acc: 0.8990, Val F1: 0.8992\nEpoch 74/100\nTrain Loss: 0.3092, Train Acc: 0.8538, Train F1: 0.8541\nVal Loss: 0.2397, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 75/100\nTrain Loss: 0.3128, Train Acc: 0.8530, Train F1: 0.8533\nVal Loss: 0.2467, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 76/100\nTrain Loss: 0.3095, Train Acc: 0.8588, Train F1: 0.8591\nVal Loss: 0.2436, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 77/100\nTrain Loss: 0.3077, Train Acc: 0.8566, Train F1: 0.8569\nVal Loss: 0.2472, Val Acc: 0.8864, Val F1: 0.8865\nEpoch 78/100\nTrain Loss: 0.3101, Train Acc: 0.8524, Train F1: 0.8527\nVal Loss: 0.2421, Val Acc: 0.8889, Val F1: 0.8890\nEpoch 79/100\nTrain Loss: 0.3126, Train Acc: 0.8545, Train F1: 0.8548\nVal Loss: 0.2412, Val Acc: 0.8990, Val F1: 0.8992\nEpoch 80/100\nTrain Loss: 0.3128, Train Acc: 0.8510, Train F1: 0.8514\nVal Loss: 0.2434, Val Acc: 0.8990, Val F1: 0.8992\nEpoch 81/100\nTrain Loss: 0.3080, Train Acc: 0.8560, Train F1: 0.8563\nVal Loss: 0.2455, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 82/100\nTrain Loss: 0.3082, Train Acc: 0.8554, Train F1: 0.8557\nVal Loss: 0.2398, Val Acc: 0.8990, Val F1: 0.8992\nEpoch 83/100\nTrain Loss: 0.3129, Train Acc: 0.8563, Train F1: 0.8566\nVal Loss: 0.2509, Val Acc: 0.8889, Val F1: 0.8890\nEpoch 84/100\nTrain Loss: 0.3112, Train Acc: 0.8530, Train F1: 0.8533\nVal Loss: 0.2422, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 85/100\nTrain Loss: 0.3138, Train Acc: 0.8529, Train F1: 0.8532\nVal Loss: 0.2464, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 86/100\nTrain Loss: 0.3094, Train Acc: 0.8541, Train F1: 0.8544\nVal Loss: 0.2397, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 87/100\nTrain Loss: 0.3129, Train Acc: 0.8544, Train F1: 0.8547\nVal Loss: 0.2366, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 88/100\nTrain Loss: 0.3071, Train Acc: 0.8527, Train F1: 0.8530\nVal Loss: 0.2447, Val Acc: 0.8914, Val F1: 0.8915\nEpoch 89/100\nTrain Loss: 0.3155, Train Acc: 0.8533, Train F1: 0.8536\nVal Loss: 0.2381, Val Acc: 0.8990, Val F1: 0.8992\nEpoch 90/100\nTrain Loss: 0.3095, Train Acc: 0.8541, Train F1: 0.8544\nVal Loss: 0.2420, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 91/100\nTrain Loss: 0.3109, Train Acc: 0.8517, Train F1: 0.8520\nVal Loss: 0.2423, Val Acc: 0.8990, Val F1: 0.8992\nEpoch 92/100\nTrain Loss: 0.3089, Train Acc: 0.8542, Train F1: 0.8545\nVal Loss: 0.2380, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 93/100\nTrain Loss: 0.3104, Train Acc: 0.8535, Train F1: 0.8539\nVal Loss: 0.2444, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 94/100\nTrain Loss: 0.3056, Train Acc: 0.8564, Train F1: 0.8567\nVal Loss: 0.2445, Val Acc: 0.8864, Val F1: 0.8865\nEpoch 95/100\nTrain Loss: 0.3068, Train Acc: 0.8586, Train F1: 0.8589\nVal Loss: 0.2394, Val Acc: 0.8864, Val F1: 0.8866\nEpoch 96/100\nTrain Loss: 0.3090, Train Acc: 0.8548, Train F1: 0.8551\nVal Loss: 0.2354, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 97/100\nTrain Loss: 0.3131, Train Acc: 0.8494, Train F1: 0.8497\nVal Loss: 0.2413, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 98/100\nTrain Loss: 0.3094, Train Acc: 0.8546, Train F1: 0.8549\nVal Loss: 0.2409, Val Acc: 0.8990, Val F1: 0.8992\nEpoch 99/100\nTrain Loss: 0.3088, Train Acc: 0.8577, Train F1: 0.8580\nVal Loss: 0.2400, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 100/100\nTrain Loss: 0.3066, Train Acc: 0.8544, Train F1: 0.8547\nVal Loss: 0.2400, Val Acc: 0.8889, Val F1: 0.8890\nBest Validation Accuracy: 0.8990\nTest Accuracy: 0.8667\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# Experiment 2 [Prev Highest Accuracy Model]","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, channels, reduction_ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        \n        self.fc = nn.Sequential(\n            conv1x1(channels, channels // reduction_ratio, bias=True),\n            nn.ReLU(inplace=True),\n            conv1x1(channels // reduction_ratio, channels, bias=True)\n        )\n        \n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = torch.sigmoid(avg_out + max_out)\n        return out\n\nclass SpatialAttention(nn.Module):\n    def __init__(self):\n        super(SpatialAttention, self).__init__()\n        self.conv = conv3x3_block(2, 1, activation=None)\n        \n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        out = torch.cat([avg_out, max_out], dim=1)\n        out = self.conv(out)\n        return torch.sigmoid(out)\n\nclass CBAMBlock(nn.Module):\n    def __init__(self, channels, reduction_ratio=16):\n        super(CBAMBlock, self).__init__()\n        self.channel_attention = ChannelAttention(channels, reduction_ratio)\n        self.spatial_attention = SpatialAttention()\n        \n    def forward(self, x):\n        out = x * self.channel_attention(x)\n        out = out * self.spatial_attention(out)\n        return out\n\nclass EnhancedDenseBlock(nn.Module):\n    def __init__(self, in_channels, growth_rate, bottleneck_ratio=4):\n        super(EnhancedDenseBlock, self).__init__()\n        bottleneck_channels = growth_rate * bottleneck_ratio\n        \n        self.branch1 = nn.Sequential(\n            conv1x1_block(in_channels, bottleneck_channels),\n            conv3x3_block(bottleneck_channels, growth_rate)\n        )\n        \n        self.branch2 = nn.Sequential(\n            conv1x1_block(in_channels, bottleneck_channels),\n            conv3x3_block(bottleneck_channels, growth_rate),\n            conv3x3_block(growth_rate, growth_rate)\n        )\n        \n        self.cbam = CBAMBlock(growth_rate * 2)\n        \n    def forward(self, x):\n        branch1 = self.branch1(x)\n        branch2 = self.branch2(x)\n        out = torch.cat([branch1, branch2], dim=1)\n        out = self.cbam(out)\n        out = torch.cat([x, out], dim=1)\n        return out\n\nclass TransitionBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(TransitionBlock, self).__init__()\n        self.conv = conv1x1_block(in_channels, out_channels)\n        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.pool(x)\n        return x\n\nclass InitialBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(InitialBlock, self).__init__()\n        self.conv1 = conv3x3_block(in_channels, out_channels // 2, stride=2)\n        self.conv2 = conv3x3_block(out_channels // 2, out_channels, stride=2)\n        self.cbam = CBAMBlock(out_channels)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.cbam(x)\n        return x\n\nclass EnhancedNetwork(nn.Module):\n    def __init__(self,\n                 in_channels=3,\n                 init_channels=32,\n                 growth_rate=32,\n                 block_config=(3, 4, 8, 6),\n                 num_classes=1000):\n        super(EnhancedNetwork, self).__init__()\n        \n        channels = init_channels\n        \n        self.features = nn.Sequential()\n        self.features.add_module(\"init_block\", InitialBlock(in_channels, channels))\n        \n        for i, num_layers in enumerate(block_config):\n            block = nn.Sequential()\n            for j in range(num_layers):\n                block.add_module(f\"dense_layer_{j+1}\", EnhancedDenseBlock(\n                    channels,\n                    growth_rate\n                ))\n                channels += growth_rate * 2\n                \n            self.features.add_module(f\"dense_block_{i+1}\", block)\n            \n            if i != len(block_config) - 1:\n                out_channels = channels // 2\n                self.features.add_module(f\"transition_{i+1}\",\n                    TransitionBlock(channels, out_channels))\n                channels = out_channels\n        \n        self.features.add_module(\"final_bn\", nn.BatchNorm2d(channels))\n        self.features.add_module(\"final_relu\", nn.ReLU(inplace=True))\n        self.features.add_module(\"final_pool\", nn.AdaptiveAvgPool2d((1, 1)))\n        \n        self.output = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(channels, num_classes)\n        )\n        \n        self._initialize_weights()\n        \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.output(x)\n        return x\n\ndef create_model_2(num_classes=2, **kwargs):\n    return EnhancedNetwork(\n        in_channels=3,\n        init_channels=32,\n        growth_rate=32,\n        block_config=(3, 4, 8, 6),\n        num_classes=num_classes,\n        **kwargs\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T10:38:17.734015Z","iopub.execute_input":"2025-01-30T10:38:17.734319Z","iopub.status.idle":"2025-01-30T10:38:17.751677Z","shell.execute_reply.started":"2025-01-30T10:38:17.734294Z","shell.execute_reply":"2025-01-30T10:38:17.750899Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Initialize model\nmodel = create_model_2() \nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(num_params)\n\n# Transfer to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Setup optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.1, patience=3\n)\n\n# Calculate class weights\nclass_counts = train_new_df['label_encoded'].value_counts()\nclass_weights = torch.FloatTensor([1/class_counts[0], 1/class_counts[1]]).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Train the model\ntrain_model(\n    model=model, \n    train_loader=train_loader, \n    val_loader=val_loader, \n    criterion=criterion, \n    optimizer=optimizer, \n    num_epochs=30\n)\n\n# Load the best model\nmodel.load_state_dict(torch.load('best_model.pth', weights_only = True))\nmodel.eval()\n\n# Evaluate on test set\ncorrect_test = 0\ntotal_test = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        correct_test += torch.sum(preds == labels)\n        total_test += labels.size(0)\n\ntest_accuracy = correct_test.double() / total_test\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T10:38:20.749409Z","iopub.execute_input":"2025-01-30T10:38:20.749692Z","iopub.status.idle":"2025-01-30T11:10:40.230774Z","shell.execute_reply.started":"2025-01-30T10:38:20.749672Z","shell.execute_reply":"2025-01-30T11:10:40.229850Z"}},"outputs":[{"name":"stdout","text":"4016344\nEpoch 1/30\nTrain Loss: 0.5299, Train Acc: 0.7223, Train F1: 0.7222\nVal Loss: 0.3749, Val Acc: 0.8030, Val F1: 0.8031\nEpoch 2/30\nTrain Loss: 0.4630, Train Acc: 0.7700, Train F1: 0.7700\nVal Loss: 0.3439, Val Acc: 0.8434, Val F1: 0.8436\nEpoch 3/30\nTrain Loss: 0.4331, Train Acc: 0.7854, Train F1: 0.7855\nVal Loss: 0.3144, Val Acc: 0.8561, Val F1: 0.8564\nEpoch 4/30\nTrain Loss: 0.4096, Train Acc: 0.7966, Train F1: 0.7967\nVal Loss: 0.3081, Val Acc: 0.8384, Val F1: 0.8375\nEpoch 5/30\nTrain Loss: 0.3983, Train Acc: 0.8043, Train F1: 0.8046\nVal Loss: 0.3304, Val Acc: 0.8510, Val F1: 0.8513\nEpoch 6/30\nTrain Loss: 0.3864, Train Acc: 0.8150, Train F1: 0.8153\nVal Loss: 0.2794, Val Acc: 0.8813, Val F1: 0.8815\nEpoch 7/30\nTrain Loss: 0.3747, Train Acc: 0.8194, Train F1: 0.8197\nVal Loss: 0.2753, Val Acc: 0.8737, Val F1: 0.8740\nEpoch 8/30\nTrain Loss: 0.3602, Train Acc: 0.8303, Train F1: 0.8306\nVal Loss: 0.2801, Val Acc: 0.8838, Val F1: 0.8839\nEpoch 9/30\nTrain Loss: 0.3554, Train Acc: 0.8306, Train F1: 0.8310\nVal Loss: 0.2581, Val Acc: 0.8737, Val F1: 0.8738\nEpoch 10/30\nTrain Loss: 0.3450, Train Acc: 0.8347, Train F1: 0.8351\nVal Loss: 0.2691, Val Acc: 0.8838, Val F1: 0.8838\nEpoch 11/30\nTrain Loss: 0.3430, Train Acc: 0.8362, Train F1: 0.8366\nVal Loss: 0.2503, Val Acc: 0.8788, Val F1: 0.8790\nEpoch 12/30\nTrain Loss: 0.3307, Train Acc: 0.8451, Train F1: 0.8454\nVal Loss: 0.2494, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 13/30\nTrain Loss: 0.3272, Train Acc: 0.8469, Train F1: 0.8472\nVal Loss: 0.2647, Val Acc: 0.8763, Val F1: 0.8765\nEpoch 14/30\nTrain Loss: 0.3265, Train Acc: 0.8458, Train F1: 0.8461\nVal Loss: 0.2928, Val Acc: 0.8561, Val F1: 0.8564\nEpoch 15/30\nTrain Loss: 0.3241, Train Acc: 0.8504, Train F1: 0.8507\nVal Loss: 0.2694, Val Acc: 0.8813, Val F1: 0.8815\nEpoch 16/30\nTrain Loss: 0.3189, Train Acc: 0.8545, Train F1: 0.8548\nVal Loss: 0.2517, Val Acc: 0.8864, Val F1: 0.8866\nLearning rate decreased from 0.000100 to 0.000010\nEpoch 17/30\nTrain Loss: 0.2911, Train Acc: 0.8639, Train F1: 0.8642\nVal Loss: 0.2575, Val Acc: 0.8838, Val F1: 0.8838\nEpoch 18/30\nTrain Loss: 0.2866, Train Acc: 0.8676, Train F1: 0.8678\nVal Loss: 0.2625, Val Acc: 0.8687, Val F1: 0.8689\nEpoch 19/30\nTrain Loss: 0.2889, Train Acc: 0.8650, Train F1: 0.8652\nVal Loss: 0.2523, Val Acc: 0.8737, Val F1: 0.8736\nEpoch 20/30\nTrain Loss: 0.2785, Train Acc: 0.8728, Train F1: 0.8731\nVal Loss: 0.2725, Val Acc: 0.8889, Val F1: 0.8884\nLearning rate decreased from 0.000010 to 0.000001\nEpoch 21/30\nTrain Loss: 0.2754, Train Acc: 0.8739, Train F1: 0.8741\nVal Loss: 0.2582, Val Acc: 0.8838, Val F1: 0.8838\nEpoch 22/30\nTrain Loss: 0.2741, Train Acc: 0.8743, Train F1: 0.8746\nVal Loss: 0.2586, Val Acc: 0.8864, Val F1: 0.8864\nEpoch 23/30\nTrain Loss: 0.2759, Train Acc: 0.8737, Train F1: 0.8740\nVal Loss: 0.2635, Val Acc: 0.8813, Val F1: 0.8814\nEpoch 24/30\nTrain Loss: 0.2774, Train Acc: 0.8758, Train F1: 0.8760\nVal Loss: 0.2559, Val Acc: 0.8813, Val F1: 0.8813\nEpoch 25/30\nTrain Loss: 0.2770, Train Acc: 0.8744, Train F1: 0.8747\nVal Loss: 0.2657, Val Acc: 0.8788, Val F1: 0.8789\nEpoch 26/30\nTrain Loss: 0.2724, Train Acc: 0.8749, Train F1: 0.8751\nVal Loss: 0.2621, Val Acc: 0.8838, Val F1: 0.8836\nEpoch 27/30\nTrain Loss: 0.2777, Train Acc: 0.8729, Train F1: 0.8732\nVal Loss: 0.2682, Val Acc: 0.8763, Val F1: 0.8765\nEpoch 28/30\nTrain Loss: 0.2783, Train Acc: 0.8744, Train F1: 0.8747\nVal Loss: 0.2788, Val Acc: 0.8864, Val F1: 0.8859\nEpoch 29/30\nTrain Loss: 0.2715, Train Acc: 0.8764, Train F1: 0.8766\nVal Loss: 0.2676, Val Acc: 0.8838, Val F1: 0.8837\nEpoch 30/30\nTrain Loss: 0.2737, Train Acc: 0.8761, Train F1: 0.8763\nVal Loss: 0.2669, Val Acc: 0.8788, Val F1: 0.8786\nBest Validation Accuracy: 0.8914\nTest Accuracy: 0.8455\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T13:26:22.043193Z","iopub.execute_input":"2025-02-02T13:26:22.043502Z","iopub.status.idle":"2025-02-02T13:26:22.058320Z","shell.execute_reply.started":"2025-02-02T13:26:22.043478Z","shell.execute_reply":"2025-02-02T13:26:22.057633Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# Experiment 3 [ EfficientNet + SqueezeNet + LSTM + CBAM]","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass ImprovedChannelAttention(nn.Module):\n    def __init__(self, in_channels, reduction_ratio=16):  \n        super(ImprovedChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1) \n        \n        self.shared_mlp = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n            nn.BatchNorm2d(in_channels // reduction_ratio),  \n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False)\n        )\n        \n    def forward(self, x):\n        avg_out = self.shared_mlp(self.avg_pool(x))\n        max_out = self.shared_mlp(self.max_pool(x))\n        return torch.sigmoid(avg_out + max_out)  \n\nclass ImprovedSpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):  \n        super(ImprovedSpatialAttention, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False),\n            nn.BatchNorm2d(1),  \n            nn.ReLU(inplace=True) \n        )\n        \n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        return torch.sigmoid(self.conv(x))\n\nclass ImprovedCBAMBlock(nn.Module):\n    def __init__(self, in_channels, reduction_ratio=16, kernel_size=7):\n        super(ImprovedCBAMBlock, self).__init__()\n        self.channel_attention = ImprovedChannelAttention(in_channels, reduction_ratio)\n        self.spatial_attention = ImprovedSpatialAttention(kernel_size)\n        \n    def forward(self, x):\n        x = x * self.channel_attention(x)\n        x = x * self.spatial_attention(x)\n        return x\n\nclass SEBlock(nn.Module):\n    def __init__(self, in_channels, reduction_ratio=16):\n        super(SEBlock, self).__init__()\n        self.squeeze = nn.AdaptiveAvgPool2d(1)\n        self.excitation = nn.Sequential(\n            nn.Linear(in_channels, in_channels // reduction_ratio),\n            nn.ReLU(inplace=True),\n            nn.Linear(in_channels // reduction_ratio, in_channels),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        b, c, _, _ = x.size()\n        squeeze = self.squeeze(x).view(b, c)\n        excitation = self.excitation(squeeze).view(b, c, 1, 1)\n        return x * excitation\n\nclass ImprovedFireBlock(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand_channels):\n        super(ImprovedFireBlock, self).__init__()\n        self.squeeze = nn.Sequential(\n            nn.Conv2d(in_channels, squeeze_channels, 1, bias=False),\n            nn.BatchNorm2d(squeeze_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.expand_1x1 = nn.Sequential(\n            nn.Conv2d(squeeze_channels, expand_channels, 1, bias=False),\n            nn.BatchNorm2d(expand_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.expand_3x3 = nn.Sequential(\n            nn.Conv2d(squeeze_channels, expand_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(expand_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.se = SEBlock(expand_channels * 2)  \n        self.cbam = ImprovedCBAMBlock(expand_channels * 2)\n        \n    def forward(self, x):\n        x = self.squeeze(x)\n        x = torch.cat([self.expand_1x1(x), self.expand_3x3(x)], 1)\n        x = self.se(x)  \n        x = self.cbam(x) \n        return x\n\nclass ImprovedResidualBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(ImprovedResidualBlock, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(in_channels)\n        )\n        self.relu = nn.ReLU(inplace=True)\n        self.se = SEBlock(in_channels) \n        \n    def forward(self, x):\n        residual = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.se(x)  \n        x += residual \n        return self.relu(x)\n\nclass EnhancedHybridNet(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3):\n        super(EnhancedHybridNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(input_channels, 32, 3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        self.fire1 = ImprovedFireBlock(32, 16, 32)\n        self.fire2 = ImprovedFireBlock(64, 24, 48)\n        self.fire3 = ImprovedFireBlock(96, 32, 64)\n        self.fire4 = ImprovedFireBlock(128, 48, 96)\n        \n        self.residual1 = ImprovedResidualBlock(192)\n        self.residual2 = ImprovedResidualBlock(192)\n        \n        self.lstm = nn.LSTM(\n            input_size=192,\n            hidden_size=96,\n            num_layers=2,\n            batch_first=True,\n            bidirectional=True,\n            dropout=0.1\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.BatchNorm1d(192),\n            nn.Dropout(0.5),\n            nn.Linear(192, 512),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(512),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n        \n        self._initialize_weights()\n        \n    def forward(self, x):\n        x = self.features(x)\n        f1 = self.fire1(x)\n        f2 = self.fire2(f1)\n        f3 = self.fire3(f2)\n        f4 = self.fire4(f3)\n        x = self.residual1(f4)\n        x = self.residual2(x)\n        batch_size, channels, height, width = x.size()\n        x_seq = x.view(batch_size, channels, -1).permute(0, 2, 1)\n        x_lstm, _ = self.lstm(x_seq)\n        x = x_lstm.permute(0, 2, 1).view(batch_size, channels, height, width)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.LSTM):\n                for name, param in m.named_parameters():\n                    if 'weight' in name:\n                        nn.init.orthogonal_(param)\n                    elif 'bias' in name:\n                        nn.init.constant_(param, 0)\n\ndef create_model_3(num_classes=1000, pretrained=False):\n    model = EnhancedHybridNet(num_classes=num_classes)\n    if pretrained:\n        raise ValueError(\"Pretrained model is not available for EnhancedHybridNet\")\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T03:52:40.324366Z","iopub.execute_input":"2025-02-03T03:52:40.324713Z","iopub.status.idle":"2025-02-03T03:52:40.345645Z","shell.execute_reply.started":"2025-02-03T03:52:40.324686Z","shell.execute_reply":"2025-02-03T03:52:40.344532Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Initialize model\nmodel = create_model_3(num_classes=2)  # Set to 2 classes\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(num_params)\n\n# Transfer to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n\n# Setup optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.1, patience=3\n)\n\n# Calculate class weights\nclass_counts = train_new_df['label_encoded'].value_counts()\nclass_weights = torch.FloatTensor([1/class_counts[0], 1/class_counts[1]]).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Train the model\ntrain_model(\n    model=model, \n    train_loader=train_loader, \n    val_loader=val_loader, \n    criterion=criterion, \n    optimizer=optimizer, \n    num_epochs=35\n)\n\n# Load the best model\nmodel.load_state_dict(torch.load('best_model.pth', weights_only = True))\nmodel.eval()\n\n# Evaluate on test set\ncorrect_test = 0\ntotal_test = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        correct_test += torch.sum(preds == labels)\n        total_test += labels.size(0)\n\ntest_accuracy = correct_test.double() / total_test\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T03:53:14.982414Z","iopub.execute_input":"2025-02-03T03:53:14.982759Z","iopub.status.idle":"2025-02-03T04:38:42.596254Z","shell.execute_reply.started":"2025-02-03T03:53:14.982730Z","shell.execute_reply":"2025-02-03T04:38:42.595094Z"}},"outputs":[{"name":"stdout","text":"1999156\nEpoch 1/35\nTrain Loss: 0.5679, Train Acc: 0.6963, Train F1: 0.6953\nVal Loss: 0.4188, Val Acc: 0.7854, Val F1: 0.7822\nEpoch 2/35\nTrain Loss: 0.5086, Train Acc: 0.7433, Train F1: 0.7427\nVal Loss: 0.3707, Val Acc: 0.8081, Val F1: 0.8063\nEpoch 3/35\nTrain Loss: 0.4806, Train Acc: 0.7614, Train F1: 0.7611\nVal Loss: 0.3484, Val Acc: 0.8182, Val F1: 0.8165\nEpoch 4/35\nTrain Loss: 0.4735, Train Acc: 0.7700, Train F1: 0.7698\nVal Loss: 0.3555, Val Acc: 0.8308, Val F1: 0.8297\nEpoch 5/35\nTrain Loss: 0.4577, Train Acc: 0.7691, Train F1: 0.7688\nVal Loss: 0.3403, Val Acc: 0.8207, Val F1: 0.8192\nEpoch 6/35\nTrain Loss: 0.4414, Train Acc: 0.7859, Train F1: 0.7860\nVal Loss: 0.3177, Val Acc: 0.8561, Val F1: 0.8559\nEpoch 7/35\nTrain Loss: 0.4239, Train Acc: 0.7898, Train F1: 0.7899\nVal Loss: 0.3315, Val Acc: 0.8232, Val F1: 0.8226\nEpoch 8/35\nTrain Loss: 0.4090, Train Acc: 0.8015, Train F1: 0.8015\nVal Loss: 0.3564, Val Acc: 0.8333, Val F1: 0.8318\nEpoch 9/35\nTrain Loss: 0.4031, Train Acc: 0.8081, Train F1: 0.8083\nVal Loss: 0.2999, Val Acc: 0.8535, Val F1: 0.8533\nEpoch 10/35\nTrain Loss: 0.3982, Train Acc: 0.8079, Train F1: 0.8081\nVal Loss: 0.2931, Val Acc: 0.8662, Val F1: 0.8664\nEpoch 11/35\nTrain Loss: 0.3902, Train Acc: 0.8125, Train F1: 0.8127\nVal Loss: 0.2834, Val Acc: 0.8712, Val F1: 0.8715\nEpoch 12/35\nTrain Loss: 0.3803, Train Acc: 0.8159, Train F1: 0.8161\nVal Loss: 0.2832, Val Acc: 0.8636, Val F1: 0.8636\nEpoch 13/35\nTrain Loss: 0.3809, Train Acc: 0.8171, Train F1: 0.8175\nVal Loss: 0.3260, Val Acc: 0.8308, Val F1: 0.8288\nEpoch 14/35\nTrain Loss: 0.3764, Train Acc: 0.8205, Train F1: 0.8208\nVal Loss: 0.2757, Val Acc: 0.8737, Val F1: 0.8739\nEpoch 15/35\nTrain Loss: 0.3670, Train Acc: 0.8284, Train F1: 0.8287\nVal Loss: 0.2881, Val Acc: 0.8611, Val F1: 0.8609\nEpoch 16/35\nTrain Loss: 0.3687, Train Acc: 0.8236, Train F1: 0.8239\nVal Loss: 0.2727, Val Acc: 0.8712, Val F1: 0.8712\nEpoch 17/35\nTrain Loss: 0.3661, Train Acc: 0.8257, Train F1: 0.8260\nVal Loss: 0.2736, Val Acc: 0.8737, Val F1: 0.8737\nEpoch 18/35\nTrain Loss: 0.3584, Train Acc: 0.8303, Train F1: 0.8306\nVal Loss: 0.3070, Val Acc: 0.8561, Val F1: 0.8559\nLearning rate decreased from 0.000100 to 0.000010\nEpoch 19/35\nTrain Loss: 0.3413, Train Acc: 0.8385, Train F1: 0.8388\nVal Loss: 0.2684, Val Acc: 0.8737, Val F1: 0.8740\nEpoch 20/35\nTrain Loss: 0.3366, Train Acc: 0.8442, Train F1: 0.8445\nVal Loss: 0.2624, Val Acc: 0.8813, Val F1: 0.8816\nEpoch 21/35\nTrain Loss: 0.3343, Train Acc: 0.8444, Train F1: 0.8447\nVal Loss: 0.2674, Val Acc: 0.8687, Val F1: 0.8689\nEpoch 22/35\nTrain Loss: 0.3342, Train Acc: 0.8436, Train F1: 0.8439\nVal Loss: 0.2690, Val Acc: 0.8763, Val F1: 0.8763\nEpoch 23/35\nTrain Loss: 0.3321, Train Acc: 0.8430, Train F1: 0.8433\nVal Loss: 0.2681, Val Acc: 0.8687, Val F1: 0.8689\nEpoch 24/35\nTrain Loss: 0.3285, Train Acc: 0.8447, Train F1: 0.8450\nVal Loss: 0.2551, Val Acc: 0.8889, Val F1: 0.8891\nEpoch 25/35\nTrain Loss: 0.3264, Train Acc: 0.8482, Train F1: 0.8485\nVal Loss: 0.2858, Val Acc: 0.8788, Val F1: 0.8790\nEpoch 26/35\nTrain Loss: 0.3240, Train Acc: 0.8477, Train F1: 0.8481\nVal Loss: 0.2600, Val Acc: 0.8813, Val F1: 0.8815\nEpoch 27/35\nTrain Loss: 0.3201, Train Acc: 0.8484, Train F1: 0.8487\nVal Loss: 0.2561, Val Acc: 0.8838, Val F1: 0.8841\nEpoch 28/35\nTrain Loss: 0.3257, Train Acc: 0.8472, Train F1: 0.8475\nVal Loss: 0.2658, Val Acc: 0.8737, Val F1: 0.8740\nLearning rate decreased from 0.000010 to 0.000001\nEpoch 29/35\nTrain Loss: 0.3344, Train Acc: 0.8433, Train F1: 0.8436\nVal Loss: 0.2608, Val Acc: 0.8813, Val F1: 0.8816\nEpoch 30/35\nTrain Loss: 0.3274, Train Acc: 0.8497, Train F1: 0.8500\nVal Loss: 0.2571, Val Acc: 0.8864, Val F1: 0.8866\nEpoch 31/35\nTrain Loss: 0.3254, Train Acc: 0.8452, Train F1: 0.8456\nVal Loss: 0.2570, Val Acc: 0.8838, Val F1: 0.8841\nEpoch 32/35\nTrain Loss: 0.3251, Train Acc: 0.8490, Train F1: 0.8493\nVal Loss: 0.2584, Val Acc: 0.8864, Val F1: 0.8865\nEpoch 33/35\nTrain Loss: 0.3239, Train Acc: 0.8523, Train F1: 0.8526\nVal Loss: 0.2609, Val Acc: 0.8838, Val F1: 0.8841\nEpoch 34/35\nTrain Loss: 0.3266, Train Acc: 0.8459, Train F1: 0.8462\nVal Loss: 0.2572, Val Acc: 0.8813, Val F1: 0.8815\nEpoch 35/35\nTrain Loss: 0.3170, Train Acc: 0.8525, Train F1: 0.8528\nVal Loss: 0.2690, Val Acc: 0.8662, Val F1: 0.8664\nBest Validation Accuracy: 0.8889\nTest Accuracy: 0.8621\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Experiment 4 [ ShuffleNet + EfficientNet + LSTM + CBAM]","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass CBAM(nn.Module):\n    def __init__(self, channels, reduction_ratio=16):\n        super(CBAM, self).__init__()\n        self.channel_attention = ChannelAttention(channels, reduction_ratio)\n        self.spatial_attention = SpatialAttention()\n\n    def forward(self, x):\n        x = self.channel_attention(x)\n        x = self.spatial_attention(x)\n        return x\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, channels, reduction_ratio):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        \n        self.fc = nn.Sequential(\n            conv1x1(channels, channels // reduction_ratio, bias=True),\n            nn.ReLU(inplace=True),\n            conv1x1(channels // reduction_ratio, channels, bias=True)\n        )\n        \n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = torch.sigmoid(avg_out + max_out)\n        return x * out\n\nclass SpatialAttention(nn.Module):\n    def __init__(self):\n        super(SpatialAttention, self).__init__()\n        self.conv = conv3x3_block(2, 1, activation=None)\n        \n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x_cat = torch.cat([avg_out, max_out], dim=1)\n        out = torch.sigmoid(self.conv(x_cat))\n        return x * out\n\nclass CustomSEBlock(nn.Module):\n    def __init__(self, channels, reduction_ratio=16):\n        super(CustomSEBlock, self).__init__()\n        mid_channels = channels // reduction_ratio\n\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.conv1 = conv1x1(channels, mid_channels, bias=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv1x1(mid_channels, channels, bias=True)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        w = self.pool(x)\n        w = self.conv1(w)\n        w = self.relu(w)\n        w = self.conv2(w)\n        w = self.sigmoid(w)\n        return x * w\n\nclass HybridBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, groups=4):\n        super(HybridBlock, self).__init__()\n        \n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.stride = stride\n        \n        split_channels = in_channels // 2\n        \n        self.effi_path = nn.Sequential(\n            conv1x1_block(split_channels, out_channels // 2),\n            depthwise_conv3x3(out_channels // 2, stride),\n            CustomSEBlock(out_channels // 2, reduction_ratio=4)\n        )\n        \n        self.shuffle_path = nn.Sequential(\n            conv1x1_block(split_channels, out_channels // 2, groups=groups),\n            ChannelShuffle(out_channels // 2, groups),\n            depthwise_conv3x3(out_channels // 2, stride)\n        )\n        \n        self.cbam = CBAM(out_channels)\n        \n        self.adjust_identity = None\n        if stride != 1 or in_channels != out_channels:\n            self.adjust_identity = conv1x1_block(\n                in_channels, out_channels, stride=stride)\n\n    def forward(self, x):\n        if self.stride == 1:\n            x1, x2 = torch.chunk(x, 2, dim=1)\n            out1 = self.effi_path(x1)\n            out2 = self.shuffle_path(x2)\n            out = torch.cat([out1, out2], dim=1)\n        else:\n            split_size = self.in_channels // 2\n            x1 = x[:, :split_size, :, :]\n            x2 = x[:, split_size:, :, :]\n            out1 = self.effi_path(x1)\n            out2 = self.shuffle_path(x2)\n            out = torch.cat([out1, out2], dim=1)\n            \n        out = self.cbam(out)\n        \n        if self.adjust_identity is not None:\n            identity = self.adjust_identity(x)\n        else:\n            identity = x\n            \n        return F.relu(out + identity)\n\nclass SpatialLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(SpatialLSTM, self).__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n        \n    def forward(self, x):\n        batch, channels, height, width = x.size()\n        x = x.permute(0, 2, 3, 1).contiguous()  # B x H x W x C\n        x = x.view(batch * height, width, channels)  # (B*H) x W x C\n        \n        x, _ = self.lstm(x)\n\n        x = x.view(batch, height, width, -1)\n        x = x.permute(0, 3, 1, 2).contiguous()\n        return x\n\nclass HybridNet(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3):\n        super(HybridNet, self).__init__()\n        self.init_conv = conv3x3_block(\n            in_channels=input_channels,\n            out_channels=64,\n            stride=2)\n        self.stage1 = self._make_stage(64, 128, blocks=3)\n        self.stage2 = self._make_stage(128, 256, blocks=4)\n        self.stage3 = self._make_stage(256, 512, blocks=6)\n        self.stage4 = self._make_stage(512, 1024, blocks=3)\n        self.spatial_lstm = SpatialLSTM(1024, 512)\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.dropout = nn.Dropout(0.2)\n        self.fc = nn.Linear(1024, num_classes)\n        \n        self._initialize_weights()\n        \n    def _make_stage(self, in_channels, out_channels, blocks):\n        layers = []\n        layers.append(HybridBlock(in_channels, out_channels, stride=2))\n        for _ in range(1, blocks):\n            layers.append(HybridBlock(out_channels, out_channels, stride=1))\n        return nn.Sequential(*layers)\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.init_conv(x)\n        \n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        \n        x = self.spatial_lstm(x)\n        \n        x = self.global_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.fc(x)\n        \n        return x\n\ndef create_model_4(num_classes=1000, input_channels=3):\n    model = HybridNet(num_classes=num_classes, input_channels=input_channels)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T04:57:50.961310Z","iopub.execute_input":"2025-02-03T04:57:50.961679Z","iopub.status.idle":"2025-02-03T04:57:50.983850Z","shell.execute_reply.started":"2025-02-03T04:57:50.961650Z","shell.execute_reply":"2025-02-03T04:57:50.982687Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Initialize model\nmodel = create_model_4(num_classes=2)  # Set to 2 classes\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(num_params)\n\n# Transfer to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Setup optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.1, patience=3\n)\n\n# Calculate class weights\nclass_counts = train_new_df['label_encoded'].value_counts()\nclass_weights = torch.FloatTensor([1/class_counts[0], 1/class_counts[1]]).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Train the model\ntrain_model(\n    model=model, \n    train_loader=train_loader, \n    val_loader=val_loader, \n    criterion=criterion, \n    optimizer=optimizer, \n    num_epochs=35\n)\n\n# Load the best model\nmodel.load_state_dict(torch.load('best_model.pth', weights_only = True))\nmodel.eval()\n\n# Evaluate on test set\ncorrect_test = 0\ntotal_test = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        correct_test += torch.sum(preds == labels)\n        total_test += labels.size(0)\n\ntest_accuracy = correct_test.double() / total_test\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T04:57:51.526094Z","iopub.execute_input":"2025-02-03T04:57:51.526417Z","iopub.status.idle":"2025-02-03T06:20:12.356319Z","shell.execute_reply.started":"2025-02-03T04:57:51.526392Z","shell.execute_reply":"2025-02-03T06:20:12.355313Z"}},"outputs":[{"name":"stdout","text":"9711562\nEpoch 1/35\nTrain Loss: 0.5497, Train Acc: 0.7115, Train F1: 0.7118\nVal Loss: 0.4047, Val Acc: 0.7929, Val F1: 0.7914\nEpoch 2/35\nTrain Loss: 0.4981, Train Acc: 0.7453, Train F1: 0.7452\nVal Loss: 0.3784, Val Acc: 0.8207, Val F1: 0.8207\nEpoch 3/35\nTrain Loss: 0.4719, Train Acc: 0.7645, Train F1: 0.7645\nVal Loss: 0.3715, Val Acc: 0.8258, Val F1: 0.8261\nEpoch 4/35\nTrain Loss: 0.4587, Train Acc: 0.7736, Train F1: 0.7736\nVal Loss: 0.3683, Val Acc: 0.8308, Val F1: 0.8310\nEpoch 5/35\nTrain Loss: 0.4501, Train Acc: 0.7757, Train F1: 0.7759\nVal Loss: 0.3575, Val Acc: 0.8283, Val F1: 0.8275\nEpoch 6/35\nTrain Loss: 0.4364, Train Acc: 0.7804, Train F1: 0.7805\nVal Loss: 0.3614, Val Acc: 0.8258, Val F1: 0.8255\nEpoch 7/35\nTrain Loss: 0.4340, Train Acc: 0.7871, Train F1: 0.7873\nVal Loss: 0.3412, Val Acc: 0.8409, Val F1: 0.8412\nEpoch 8/35\nTrain Loss: 0.4276, Train Acc: 0.7912, Train F1: 0.7913\nVal Loss: 0.3316, Val Acc: 0.8510, Val F1: 0.8513\nEpoch 9/35\nTrain Loss: 0.4233, Train Acc: 0.7935, Train F1: 0.7937\nVal Loss: 0.3417, Val Acc: 0.8510, Val F1: 0.8512\nEpoch 10/35\nTrain Loss: 0.4127, Train Acc: 0.7962, Train F1: 0.7963\nVal Loss: 0.3321, Val Acc: 0.8359, Val F1: 0.8361\nEpoch 11/35\nTrain Loss: 0.4060, Train Acc: 0.8021, Train F1: 0.8023\nVal Loss: 0.3167, Val Acc: 0.8510, Val F1: 0.8512\nEpoch 12/35\nTrain Loss: 0.4067, Train Acc: 0.7993, Train F1: 0.7995\nVal Loss: 0.3088, Val Acc: 0.8611, Val F1: 0.8607\nEpoch 13/35\nTrain Loss: 0.3993, Train Acc: 0.8029, Train F1: 0.8032\nVal Loss: 0.3086, Val Acc: 0.8687, Val F1: 0.8689\nEpoch 14/35\nTrain Loss: 0.3870, Train Acc: 0.8106, Train F1: 0.8109\nVal Loss: 0.3157, Val Acc: 0.8636, Val F1: 0.8637\nEpoch 15/35\nTrain Loss: 0.3821, Train Acc: 0.8128, Train F1: 0.8131\nVal Loss: 0.3115, Val Acc: 0.8611, Val F1: 0.8614\nEpoch 16/35\nTrain Loss: 0.3774, Train Acc: 0.8165, Train F1: 0.8168\nVal Loss: 0.3009, Val Acc: 0.8763, Val F1: 0.8765\nEpoch 17/35\nTrain Loss: 0.3737, Train Acc: 0.8170, Train F1: 0.8172\nVal Loss: 0.2857, Val Acc: 0.8636, Val F1: 0.8639\nEpoch 18/35\nTrain Loss: 0.3678, Train Acc: 0.8198, Train F1: 0.8201\nVal Loss: 0.2944, Val Acc: 0.8636, Val F1: 0.8636\nEpoch 19/35\nTrain Loss: 0.3665, Train Acc: 0.8163, Train F1: 0.8166\nVal Loss: 0.2881, Val Acc: 0.8662, Val F1: 0.8660\nEpoch 20/35\nTrain Loss: 0.3579, Train Acc: 0.8245, Train F1: 0.8249\nVal Loss: 0.2911, Val Acc: 0.8763, Val F1: 0.8762\nLearning rate decreased from 0.000100 to 0.000010\nEpoch 21/35\nTrain Loss: 0.3400, Train Acc: 0.8302, Train F1: 0.8304\nVal Loss: 0.2694, Val Acc: 0.9015, Val F1: 0.9017\nEpoch 22/35\nTrain Loss: 0.3321, Train Acc: 0.8357, Train F1: 0.8360\nVal Loss: 0.2651, Val Acc: 0.9040, Val F1: 0.9042\nEpoch 23/35\nTrain Loss: 0.3332, Train Acc: 0.8381, Train F1: 0.8384\nVal Loss: 0.2634, Val Acc: 0.9040, Val F1: 0.9042\nEpoch 24/35\nTrain Loss: 0.3261, Train Acc: 0.8392, Train F1: 0.8395\nVal Loss: 0.2640, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 25/35\nTrain Loss: 0.3288, Train Acc: 0.8399, Train F1: 0.8402\nVal Loss: 0.2677, Val Acc: 0.9015, Val F1: 0.9017\nEpoch 26/35\nTrain Loss: 0.3291, Train Acc: 0.8386, Train F1: 0.8390\nVal Loss: 0.2634, Val Acc: 0.9040, Val F1: 0.9042\nLearning rate decreased from 0.000010 to 0.000001\nEpoch 27/35\nTrain Loss: 0.3248, Train Acc: 0.8448, Train F1: 0.8451\nVal Loss: 0.2619, Val Acc: 0.8990, Val F1: 0.8992\nEpoch 28/35\nTrain Loss: 0.3255, Train Acc: 0.8390, Train F1: 0.8393\nVal Loss: 0.2627, Val Acc: 0.9015, Val F1: 0.9017\nEpoch 29/35\nTrain Loss: 0.3216, Train Acc: 0.8448, Train F1: 0.8451\nVal Loss: 0.2654, Val Acc: 0.9015, Val F1: 0.9017\nEpoch 30/35\nTrain Loss: 0.3258, Train Acc: 0.8431, Train F1: 0.8434\nVal Loss: 0.2659, Val Acc: 0.9040, Val F1: 0.9042\nEpoch 31/35\nTrain Loss: 0.3242, Train Acc: 0.8440, Train F1: 0.8443\nVal Loss: 0.2676, Val Acc: 0.9066, Val F1: 0.9067\nEpoch 32/35\nTrain Loss: 0.3233, Train Acc: 0.8398, Train F1: 0.8401\nVal Loss: 0.2626, Val Acc: 0.9066, Val F1: 0.9068\nEpoch 33/35\nTrain Loss: 0.3264, Train Acc: 0.8429, Train F1: 0.8433\nVal Loss: 0.2631, Val Acc: 0.9040, Val F1: 0.9042\nEpoch 34/35\nTrain Loss: 0.3201, Train Acc: 0.8444, Train F1: 0.8448\nVal Loss: 0.2674, Val Acc: 0.9040, Val F1: 0.9042\nEpoch 35/35\nTrain Loss: 0.3202, Train Acc: 0.8442, Train F1: 0.8445\nVal Loss: 0.2644, Val Acc: 0.9040, Val F1: 0.9042\nBest Validation Accuracy: 0.9066\nTest Accuracy: 0.8621\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Experiment 4 [ ShuffleNet + EfficientNet + LSTM + CBAM] Less Param Ver.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        \n        self.fc = nn.Sequential(\n            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n        )\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        self.conv = conv3x3_block(2, 1, activation=None)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv(x)\n        return self.sigmoid(x)\n\nclass CBAM(nn.Module):\n    def __init__(self, channels, reduction=16, kernel_size=7):\n        super(CBAM, self).__init__()\n        self.channel_att = ChannelAttention(channels, reduction)\n        self.spatial_att = SpatialAttention(kernel_size)\n\n    def forward(self, x):\n        x = x * self.channel_att(x)\n        x = x * self.spatial_att(x)\n        return x\n\nclass LightweightShuffle(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(LightweightShuffle, self).__init__()\n        self.stride = stride\n        mid_channels = out_channels // 4\n        \n        self.conv1 = conv1x1_block(in_channels, mid_channels)\n        self.shuffle = ChannelShuffle(mid_channels, 2)\n        self.dwconv = dwconv3x3_block(mid_channels, mid_channels, stride=stride)\n        self.conv2 = conv1x1_block(mid_channels, out_channels)\n        \n        if stride == 2 or in_channels != out_channels:\n            self.shortcut = conv1x1_block(in_channels, out_channels, stride=stride)\n        else:\n            self.shortcut = None\n            \n    def forward(self, x):\n        residual = x\n        \n        out = self.conv1(x)\n        out = self.shuffle(out)\n        out = self.dwconv(out)\n        out = self.conv2(out)\n        \n        if self.shortcut:\n            residual = self.shortcut(x)\n            \n        return out + residual\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, channels):\n        super(TemporalBlock, self).__init__()\n        self.lstm = nn.LSTM(channels, channels // 2, bidirectional=True, batch_first=True)\n        \n    def forward(self, x):\n        b, c, h, w = x.size()\n        x = x.view(b, c, -1).permute(0, 2, 1) \n        x, _ = self.lstm(x)\n        x = x.permute(0, 2, 1).view(b, c, h, w)\n        return x\n\nclass HybridBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, reduction=16):\n        super(HybridBlock, self).__init__()\n        self.shuffle = LightweightShuffle(in_channels, out_channels, stride)\n        self.cbam = CBAM(out_channels, reduction)\n        self.temporal = TemporalBlock(out_channels)\n        \n    def forward(self, x):\n        x = self.shuffle(x)\n        x = self.cbam(x)\n        x = self.temporal(x)\n        return x\n\nclass HybridNet(nn.Module):\n    def __init__(self, num_classes=1000, input_size=224):\n        super(HybridNet, self).__init__()\n        \n        # Initial conv block\n        self.init_block = conv3x3_block(3, 32, stride=2)\n        \n        # Main stages\n        self.stage1 = self._make_stage(32, 64, blocks=2)\n        self.stage2 = self._make_stage(64, 128, blocks=3)\n        self.stage3 = self._make_stage(128, 256, blocks=4)\n        self.stage4 = self._make_stage(256, 512, blocks=3)\n        \n        # Final layers\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.dropout = nn.Dropout(0.2)\n        self.fc = nn.Linear(512, num_classes)\n        \n        self._initialize_weights()\n        \n    def _make_stage(self, in_channels, out_channels, blocks):\n        layers = []\n        layers.append(HybridBlock(in_channels, out_channels, stride=2))\n        for _ in range(1, blocks):\n            layers.append(HybridBlock(out_channels, out_channels, stride=1))\n        return nn.Sequential(*layers)\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n                \n    def forward(self, x):\n        x = self.init_block(x)\n        \n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        \n        x = self.global_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.fc(x)\n        \n        return x\n\ndef create_model_4_0(num_classes=1000, input_size=224):\n    return HybridNet(num_classes=num_classes, input_size=input_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T06:20:12.363093Z","iopub.execute_input":"2025-02-03T06:20:12.363349Z","iopub.status.idle":"2025-02-03T06:20:12.384913Z","shell.execute_reply.started":"2025-02-03T06:20:12.363325Z","shell.execute_reply":"2025-02-03T06:20:12.383859Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Initialize model\nmodel = create_model_4_0(num_classes=2)  \nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(num_params)\n\n# Transfer to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Setup optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.1, patience=3\n)\n\n# Calculate class weights\nclass_counts = train_new_df['label_encoded'].value_counts()\nclass_weights = torch.FloatTensor([1/class_counts[0], 1/class_counts[1]]).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Train the model\ntrain_model(\n    model=model, \n    train_loader=train_loader, \n    val_loader=val_loader, \n    criterion=criterion, \n    optimizer=optimizer, \n    num_epochs=35\n)\n\n# Load the best model\nmodel.load_state_dict(torch.load('best_model.pth', weights_only = True))\nmodel.eval()\n\n# Evaluate on test set\ncorrect_test = 0\ntotal_test = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        correct_test += torch.sum(preds == labels)\n        total_test += labels.size(0)\n\ntest_accuracy = correct_test.double() / total_test\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T06:20:12.386207Z","iopub.execute_input":"2025-02-03T06:20:12.386574Z","iopub.status.idle":"2025-02-03T07:05:28.513849Z","shell.execute_reply.started":"2025-02-03T06:20:12.386531Z","shell.execute_reply":"2025-02-03T07:05:28.512839Z"}},"outputs":[{"name":"stdout","text":"7502098\nEpoch 1/35\nTrain Loss: 0.6379, Train Acc: 0.6301, Train F1: 0.6309\nVal Loss: 0.4980, Val Acc: 0.7652, Val F1: 0.7654\nEpoch 2/35\nTrain Loss: 0.5714, Train Acc: 0.7016, Train F1: 0.7011\nVal Loss: 0.4834, Val Acc: 0.7677, Val F1: 0.7677\nEpoch 3/35\nTrain Loss: 0.5432, Train Acc: 0.7158, Train F1: 0.7156\nVal Loss: 0.4513, Val Acc: 0.8030, Val F1: 0.8030\nEpoch 4/35\nTrain Loss: 0.5369, Train Acc: 0.7235, Train F1: 0.7229\nVal Loss: 0.4425, Val Acc: 0.7980, Val F1: 0.7983\nEpoch 5/35\nTrain Loss: 0.5241, Train Acc: 0.7338, Train F1: 0.7336\nVal Loss: 0.4268, Val Acc: 0.8106, Val F1: 0.8109\nEpoch 6/35\nTrain Loss: 0.5203, Train Acc: 0.7356, Train F1: 0.7354\nVal Loss: 0.4188, Val Acc: 0.8232, Val F1: 0.8233\nEpoch 7/35\nTrain Loss: 0.5089, Train Acc: 0.7432, Train F1: 0.7430\nVal Loss: 0.3977, Val Acc: 0.8207, Val F1: 0.8211\nEpoch 8/35\nTrain Loss: 0.4973, Train Acc: 0.7527, Train F1: 0.7526\nVal Loss: 0.3907, Val Acc: 0.8182, Val F1: 0.8174\nEpoch 9/35\nTrain Loss: 0.4890, Train Acc: 0.7557, Train F1: 0.7560\nVal Loss: 0.3817, Val Acc: 0.8308, Val F1: 0.8312\nEpoch 10/35\nTrain Loss: 0.4822, Train Acc: 0.7573, Train F1: 0.7569\nVal Loss: 0.3911, Val Acc: 0.8182, Val F1: 0.8181\nEpoch 11/35\nTrain Loss: 0.4776, Train Acc: 0.7608, Train F1: 0.7608\nVal Loss: 0.3558, Val Acc: 0.8182, Val F1: 0.8183\nEpoch 12/35\nTrain Loss: 0.4716, Train Acc: 0.7621, Train F1: 0.7621\nVal Loss: 0.3528, Val Acc: 0.8207, Val F1: 0.8208\nEpoch 13/35\nTrain Loss: 0.4609, Train Acc: 0.7717, Train F1: 0.7719\nVal Loss: 0.3558, Val Acc: 0.8283, Val F1: 0.8261\nLearning rate decreased from 0.000100 to 0.000010\nEpoch 14/35\nTrain Loss: 0.4590, Train Acc: 0.7764, Train F1: 0.7769\nVal Loss: 0.3316, Val Acc: 0.8384, Val F1: 0.8387\nEpoch 15/35\nTrain Loss: 0.4436, Train Acc: 0.7798, Train F1: 0.7801\nVal Loss: 0.3208, Val Acc: 0.8586, Val F1: 0.8589\nEpoch 16/35\nTrain Loss: 0.4458, Train Acc: 0.7849, Train F1: 0.7852\nVal Loss: 0.3233, Val Acc: 0.8434, Val F1: 0.8435\nEpoch 17/35\nTrain Loss: 0.4437, Train Acc: 0.7809, Train F1: 0.7812\nVal Loss: 0.3236, Val Acc: 0.8535, Val F1: 0.8538\nEpoch 18/35\nTrain Loss: 0.4391, Train Acc: 0.7868, Train F1: 0.7870\nVal Loss: 0.3225, Val Acc: 0.8687, Val F1: 0.8688\nEpoch 19/35\nTrain Loss: 0.4405, Train Acc: 0.7859, Train F1: 0.7862\nVal Loss: 0.3177, Val Acc: 0.8535, Val F1: 0.8538\nEpoch 20/35\nTrain Loss: 0.4387, Train Acc: 0.7849, Train F1: 0.7852\nVal Loss: 0.3261, Val Acc: 0.8586, Val F1: 0.8587\nEpoch 21/35\nTrain Loss: 0.4358, Train Acc: 0.7903, Train F1: 0.7906\nVal Loss: 0.3205, Val Acc: 0.8561, Val F1: 0.8563\nEpoch 22/35\nTrain Loss: 0.4402, Train Acc: 0.7841, Train F1: 0.7843\nVal Loss: 0.3132, Val Acc: 0.8561, Val F1: 0.8563\nLearning rate decreased from 0.000010 to 0.000001\nEpoch 23/35\nTrain Loss: 0.4317, Train Acc: 0.7923, Train F1: 0.7926\nVal Loss: 0.3119, Val Acc: 0.8636, Val F1: 0.8639\nEpoch 24/35\nTrain Loss: 0.4303, Train Acc: 0.7901, Train F1: 0.7904\nVal Loss: 0.3302, Val Acc: 0.8510, Val F1: 0.8512\nEpoch 25/35\nTrain Loss: 0.4303, Train Acc: 0.7905, Train F1: 0.7909\nVal Loss: 0.3150, Val Acc: 0.8434, Val F1: 0.8437\nEpoch 26/35\nTrain Loss: 0.4331, Train Acc: 0.7864, Train F1: 0.7868\nVal Loss: 0.3095, Val Acc: 0.8611, Val F1: 0.8614\nEpoch 27/35\nTrain Loss: 0.4397, Train Acc: 0.7830, Train F1: 0.7834\nVal Loss: 0.3120, Val Acc: 0.8687, Val F1: 0.8689\nEpoch 28/35\nTrain Loss: 0.4340, Train Acc: 0.7890, Train F1: 0.7893\nVal Loss: 0.3165, Val Acc: 0.8611, Val F1: 0.8614\nEpoch 29/35\nTrain Loss: 0.4319, Train Acc: 0.7887, Train F1: 0.7890\nVal Loss: 0.3136, Val Acc: 0.8611, Val F1: 0.8614\nEpoch 30/35\nTrain Loss: 0.4244, Train Acc: 0.7961, Train F1: 0.7964\nVal Loss: 0.3037, Val Acc: 0.8586, Val F1: 0.8589\nEpoch 31/35\nTrain Loss: 0.4375, Train Acc: 0.7842, Train F1: 0.7846\nVal Loss: 0.3179, Val Acc: 0.8409, Val F1: 0.8411\nEpoch 32/35\nTrain Loss: 0.4249, Train Acc: 0.7954, Train F1: 0.7958\nVal Loss: 0.3280, Val Acc: 0.8586, Val F1: 0.8586\nEpoch 33/35\nTrain Loss: 0.4319, Train Acc: 0.7897, Train F1: 0.7901\nVal Loss: 0.3221, Val Acc: 0.8535, Val F1: 0.8537\nEpoch 34/35\nTrain Loss: 0.4280, Train Acc: 0.7912, Train F1: 0.7915\nVal Loss: 0.3067, Val Acc: 0.8561, Val F1: 0.8563\nEpoch 35/35\nTrain Loss: 0.4285, Train Acc: 0.7938, Train F1: 0.7942\nVal Loss: 0.3122, Val Acc: 0.8586, Val F1: 0.8587\nBest Validation Accuracy: 0.8687\nTest Accuracy: 0.8152\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# Experiment 5 [ SeResNet + OctResNet + CBAM + SE Block]","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EfficientAttention(nn.Module):\n    def __init__(self, channels):\n        super(EfficientAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        reduced_channels = max(channels // 16, 4)\n        self.fc = nn.Sequential(\n            nn.Conv2d(channels, reduced_channels, 1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(reduced_channels, channels, 1, bias=False),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        return x * self.fc(self.avg_pool(x))\n\nclass DepthwiseSeparableConv(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(DepthwiseSeparableConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels, bias=False),\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.conv(x)\n\nclass UltraLightBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(UltraLightBlock, self).__init__()\n        hidden_channels = out_channels // 2\n        \n        self.conv1 = DepthwiseSeparableConv(in_channels, hidden_channels, stride)\n        self.conv2 = DepthwiseSeparableConv(hidden_channels, out_channels)\n        self.attention = EfficientAttention(out_channels) if out_channels >= 64 else None\n        \n        self.shortcut = None\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n            \n    def forward(self, x):\n        identity = x\n        \n        out = self.conv1(x)\n        out = self.conv2(out)\n        \n        if self.shortcut is not None:\n            identity = self.shortcut(x)\n            \n        out = out + identity\n        if self.attention is not None:\n            out = self.attention(out)\n        \n        return out\n\nclass UltraLightHybridNet(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3):\n        super(UltraLightHybridNet, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(input_channels, 16, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(16),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.layer1 = self._make_layer(16, 32, blocks=1, stride=1)\n        self.layer2 = self._make_layer(32, 64, blocks=2, stride=2)\n        self.layer3 = self._make_layer(64, 128, blocks=2, stride=2)\n        self.layer4 = self._make_layer(128, 256, blocks=1, stride=2)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(128, num_classes)\n        )\n        \n        self._initialize_weights()\n        \n    def _make_layer(self, in_channels, out_channels, blocks, stride):\n        layers = []\n        layers.append(UltraLightBlock(in_channels, out_channels, stride=stride))\n        for _ in range(1, blocks):\n            layers.append(UltraLightBlock(out_channels, out_channels))\n        return nn.Sequential(*layers)\n        \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n                \n    def forward(self, x):\n        x = self.conv1(x)\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        \n        return x\n\ndef create_model_5(num_classes=1000, **kwargs):\n    model = UltraLightHybridNet(num_classes=num_classes, **kwargs)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:53:48.253729Z","iopub.execute_input":"2025-02-03T08:53:48.254071Z","iopub.status.idle":"2025-02-03T08:53:48.269236Z","shell.execute_reply.started":"2025-02-03T08:53:48.254042Z","shell.execute_reply":"2025-02-03T08:53:48.268564Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Initialize model\nmodel = create_model_5(num_classes=2)  # Set to 2 classes\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(num_params)\n\n# Transfer to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Setup optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.1, patience=3\n)\n\n# Calculate class weights\nclass_counts = train_new_df['label_encoded'].value_counts()\nclass_weights = torch.FloatTensor([1/class_counts[0], 1/class_counts[1]]).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Train the model\ntrain_model(\n    model=model, \n    train_loader=train_loader, \n    val_loader=val_loader, \n    criterion=criterion, \n    optimizer=optimizer, \n    num_epochs=35\n)\n\n# Load the best model\nmodel.load_state_dict(torch.load('best_model.pth', weights_only = True))\nmodel.eval()\n\n# Evaluate on test set\ncorrect_test = 0\ntotal_test = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        correct_test += torch.sum(preds == labels)\n        total_test += labels.size(0)\n\ntest_accuracy = correct_test.double() / total_test\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:53:49.909404Z","iopub.execute_input":"2025-02-03T08:53:49.909682Z","iopub.status.idle":"2025-02-03T09:19:43.503382Z","shell.execute_reply.started":"2025-02-03T08:53:49.909662Z","shell.execute_reply":"2025-02-03T09:19:43.502436Z"}},"outputs":[{"name":"stdout","text":"187634\nEpoch 1/35\nTrain Loss: 0.5527, Train Acc: 0.7146, Train F1: 0.7150\nVal Loss: 0.3938, Val Acc: 0.8157, Val F1: 0.8160\nEpoch 2/35\nTrain Loss: 0.4824, Train Acc: 0.7551, Train F1: 0.7547\nVal Loss: 0.3615, Val Acc: 0.8131, Val F1: 0.8124\nEpoch 3/35\nTrain Loss: 0.4651, Train Acc: 0.7616, Train F1: 0.7613\nVal Loss: 0.3587, Val Acc: 0.8182, Val F1: 0.8184\nEpoch 4/35\nTrain Loss: 0.4571, Train Acc: 0.7697, Train F1: 0.7695\nVal Loss: 0.3616, Val Acc: 0.8434, Val F1: 0.8434\nEpoch 5/35\nTrain Loss: 0.4489, Train Acc: 0.7773, Train F1: 0.7772\nVal Loss: 0.3449, Val Acc: 0.8384, Val F1: 0.8387\nEpoch 6/35\nTrain Loss: 0.4371, Train Acc: 0.7813, Train F1: 0.7814\nVal Loss: 0.3508, Val Acc: 0.8308, Val F1: 0.8308\nEpoch 7/35\nTrain Loss: 0.4352, Train Acc: 0.7847, Train F1: 0.7849\nVal Loss: 0.3167, Val Acc: 0.8535, Val F1: 0.8537\nEpoch 8/35\nTrain Loss: 0.4274, Train Acc: 0.7870, Train F1: 0.7870\nVal Loss: 0.3103, Val Acc: 0.8712, Val F1: 0.8714\nEpoch 9/35\nTrain Loss: 0.4189, Train Acc: 0.7930, Train F1: 0.7932\nVal Loss: 0.3235, Val Acc: 0.8662, Val F1: 0.8660\nEpoch 10/35\nTrain Loss: 0.4065, Train Acc: 0.7988, Train F1: 0.7991\nVal Loss: 0.3057, Val Acc: 0.8485, Val F1: 0.8487\nEpoch 11/35\nTrain Loss: 0.3991, Train Acc: 0.8068, Train F1: 0.8071\nVal Loss: 0.2974, Val Acc: 0.8535, Val F1: 0.8534\nEpoch 12/35\nTrain Loss: 0.3962, Train Acc: 0.8071, Train F1: 0.8074\nVal Loss: 0.2976, Val Acc: 0.8535, Val F1: 0.8537\nLearning rate decreased from 0.000100 to 0.000010\nEpoch 13/35\nTrain Loss: 0.3898, Train Acc: 0.8073, Train F1: 0.8075\nVal Loss: 0.3018, Val Acc: 0.8561, Val F1: 0.8564\nEpoch 14/35\nTrain Loss: 0.3827, Train Acc: 0.8114, Train F1: 0.8117\nVal Loss: 0.2929, Val Acc: 0.8586, Val F1: 0.8589\nEpoch 15/35\nTrain Loss: 0.3829, Train Acc: 0.8104, Train F1: 0.8107\nVal Loss: 0.2886, Val Acc: 0.8636, Val F1: 0.8639\nEpoch 16/35\nTrain Loss: 0.3821, Train Acc: 0.8142, Train F1: 0.8145\nVal Loss: 0.2935, Val Acc: 0.8712, Val F1: 0.8715\nLearning rate decreased from 0.000010 to 0.000001\nEpoch 17/35\nTrain Loss: 0.3836, Train Acc: 0.8158, Train F1: 0.8161\nVal Loss: 0.2889, Val Acc: 0.8636, Val F1: 0.8639\nEpoch 18/35\nTrain Loss: 0.3776, Train Acc: 0.8174, Train F1: 0.8177\nVal Loss: 0.2837, Val Acc: 0.8636, Val F1: 0.8639\nEpoch 19/35\nTrain Loss: 0.3775, Train Acc: 0.8161, Train F1: 0.8164\nVal Loss: 0.2888, Val Acc: 0.8662, Val F1: 0.8664\nEpoch 20/35\nTrain Loss: 0.3769, Train Acc: 0.8199, Train F1: 0.8202\nVal Loss: 0.2918, Val Acc: 0.8687, Val F1: 0.8690\nEpoch 21/35\nTrain Loss: 0.3730, Train Acc: 0.8168, Train F1: 0.8170\nVal Loss: 0.2829, Val Acc: 0.8636, Val F1: 0.8639\nEpoch 22/35\nTrain Loss: 0.3710, Train Acc: 0.8202, Train F1: 0.8204\nVal Loss: 0.2934, Val Acc: 0.8611, Val F1: 0.8614\nEpoch 23/35\nTrain Loss: 0.3742, Train Acc: 0.8198, Train F1: 0.8201\nVal Loss: 0.2874, Val Acc: 0.8586, Val F1: 0.8589\nEpoch 24/35\nTrain Loss: 0.3784, Train Acc: 0.8166, Train F1: 0.8169\nVal Loss: 0.3006, Val Acc: 0.8662, Val F1: 0.8664\nEpoch 25/35\nTrain Loss: 0.3775, Train Acc: 0.8166, Train F1: 0.8169\nVal Loss: 0.2897, Val Acc: 0.8687, Val F1: 0.8688\nEpoch 26/35\nTrain Loss: 0.3733, Train Acc: 0.8206, Train F1: 0.8209\nVal Loss: 0.2891, Val Acc: 0.8611, Val F1: 0.8614\nEpoch 27/35\nTrain Loss: 0.3766, Train Acc: 0.8195, Train F1: 0.8198\nVal Loss: 0.2981, Val Acc: 0.8586, Val F1: 0.8588\nEpoch 28/35\nTrain Loss: 0.3819, Train Acc: 0.8142, Train F1: 0.8145\nVal Loss: 0.2838, Val Acc: 0.8737, Val F1: 0.8740\nEpoch 29/35\nTrain Loss: 0.3798, Train Acc: 0.8177, Train F1: 0.8180\nVal Loss: 0.2852, Val Acc: 0.8636, Val F1: 0.8639\nEpoch 30/35\nTrain Loss: 0.3816, Train Acc: 0.8141, Train F1: 0.8144\nVal Loss: 0.2913, Val Acc: 0.8737, Val F1: 0.8740\nEpoch 31/35\nTrain Loss: 0.3768, Train Acc: 0.8169, Train F1: 0.8172\nVal Loss: 0.2909, Val Acc: 0.8712, Val F1: 0.8714\nEpoch 32/35\nTrain Loss: 0.3776, Train Acc: 0.8150, Train F1: 0.8153\nVal Loss: 0.2928, Val Acc: 0.8636, Val F1: 0.8639\nEpoch 33/35\nTrain Loss: 0.3707, Train Acc: 0.8185, Train F1: 0.8188\nVal Loss: 0.2977, Val Acc: 0.8636, Val F1: 0.8638\nEpoch 34/35\nTrain Loss: 0.3718, Train Acc: 0.8223, Train F1: 0.8226\nVal Loss: 0.2932, Val Acc: 0.8561, Val F1: 0.8564\nEpoch 35/35\nTrain Loss: 0.3703, Train Acc: 0.8219, Train F1: 0.8221\nVal Loss: 0.2844, Val Acc: 0.8636, Val F1: 0.8639\nBest Validation Accuracy: 0.8737\nTest Accuracy: 0.8470\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# Experiment 6 [ SeResNet + PeleeNet + CBAM]","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nfrom torch.nn import init\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, channels, reduction_ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        \n        mid_channels = max(8, channels // reduction_ratio)\n        \n        self.shared_mlp = nn.Sequential(\n            conv1x1(channels, mid_channels, bias=True),\n            nn.ReLU(inplace=True),\n            conv1x1(mid_channels, channels, bias=True)\n        )\n        \n    def forward(self, x):\n        avg_out = self.shared_mlp(self.avg_pool(x))\n        max_out = self.shared_mlp(self.max_pool(x))\n        out = torch.sigmoid(avg_out + max_out)\n        return out\n\nclass SpatialAttention(nn.Module):\n    def __init__(self):\n        super(SpatialAttention, self).__init__()\n        self.conv = conv3x3_block(2, 1, activation=None)\n        \n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv(x)\n        return torch.sigmoid(x)\n\nclass LightCBAM(nn.Module):\n    def __init__(self, channels):\n        super(LightCBAM, self).__init__()\n        self.channel_att = ChannelAttention(channels)\n        self.spatial_att = SpatialAttention()\n        \n    def forward(self, x):\n        x = x * self.channel_att(x)\n        x = x * self.spatial_att(x)\n        return x\n\nclass LightResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(LightResBlock, self).__init__()\n        \n        self.conv1 = conv3x3_block(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            stride=stride)\n            \n        self.conv2 = conv3x3_block(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            activation=None)\n            \n        self.attention = LightCBAM(out_channels)\n        \n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = conv1x1_block(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                stride=stride,\n                activation=None)\n        else:\n            self.shortcut = None\n            \n        self.activ = nn.ReLU(inplace=True)\n        \n    def forward(self, x):\n        identity = x if self.shortcut is None else self.shortcut(x)\n        \n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention(x)\n        \n        x = x + identity\n        x = self.activ(x)\n        return x\n\nclass DenseFeatureBlock(nn.Module):\n    def __init__(self, in_channels, growth_rate, bottleneck_ratio=2):\n        super(DenseFeatureBlock, self).__init__()\n        \n        bottleneck_channels = growth_rate * bottleneck_ratio\n        \n        self.branch1 = nn.Sequential(\n            conv1x1_block(in_channels, bottleneck_channels),\n            conv3x3_block(bottleneck_channels, growth_rate)\n        )\n        \n        self.branch2 = nn.Sequential(\n            conv1x1_block(in_channels, bottleneck_channels),\n            conv3x3_block(bottleneck_channels, growth_rate),\n            conv3x3_block(growth_rate, growth_rate)\n        )\n        \n    def forward(self, x):\n        branch1 = self.branch1(x)\n        branch2 = self.branch2(x)\n        return torch.cat([x, branch1, branch2], dim=1)\n\nclass TransitionBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(TransitionBlock, self).__init__()\n        self.conv = conv1x1_block(in_channels, out_channels)\n        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.pool(x)\n        return x\n\nclass LightAttentionNet(nn.Module):\n    def __init__(self,\n                 channels=[32, 64, 128, 256],\n                 init_channels=32,\n                 growth_rates=[16, 24, 32, 48],\n                 num_classes=1000):\n        super(LightAttentionNet, self).__init__()\n        \n        self.features = nn.Sequential()\n        self.features.add_module(\"init_conv\", conv3x3_block(\n            in_channels=3,\n            out_channels=init_channels,\n            stride=2))\n        \n        self.features.add_module(\"init_pool\", nn.MaxPool2d(\n            kernel_size=3,\n            stride=2,\n            padding=1))\n            \n        in_channels = init_channels\n        \n        for i, (out_channels, growth_rate) in enumerate(zip(channels, growth_rates)):\n            stage = nn.Sequential()\n            \n            for j in range(i + 1):\n                stage.add_module(f\"dense{j + 1}\", DenseFeatureBlock(\n                    in_channels=in_channels,\n                    growth_rate=growth_rate))\n                in_channels += growth_rate * 2\n            \n            stage.add_module(\"res_att\", LightResBlock(\n                in_channels=in_channels,\n                out_channels=out_channels))\n            \n            if i != len(channels) - 1:\n                stage.add_module(\"transition\", TransitionBlock(\n                    in_channels=out_channels,\n                    out_channels=out_channels))\n            \n            self.features.add_module(f\"stage{i + 1}\", stage)\n            in_channels = out_channels\n        \n        self.features.add_module(\"final_pool\", nn.AdaptiveAvgPool2d(1))\n        \n        self.output = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(in_channels, num_classes)\n        )\n        \n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal_(m.weight, std=0.001)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.output(x)\n        return x\n\ndef create_model_6(num_classes=1000, **kwargs):\n    return LightAttentionNet(num_classes=num_classes, **kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:14:09.393475Z","iopub.execute_input":"2025-02-03T14:14:09.393868Z","iopub.status.idle":"2025-02-03T14:14:09.413053Z","shell.execute_reply.started":"2025-02-03T14:14:09.393798Z","shell.execute_reply":"2025-02-03T14:14:09.412173Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Initialize model\nmodel = create_model_6(num_classes=2)  # Set to 2 classes\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(num_params)\n\n# Transfer to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Setup optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.1, patience=3\n)\n\n# Calculate class weights\nclass_counts = train_new_df['label_encoded'].value_counts()\nclass_weights = torch.FloatTensor([1/class_counts[0], 1/class_counts[1]]).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Train the model\ntrain_model(\n    model=model, \n    train_loader=train_loader, \n    val_loader=val_loader, \n    criterion=criterion, \n    optimizer=optimizer, \n    num_epochs=35\n)\n\n# Load the best model\nmodel.load_state_dict(torch.load('best_model.pth', weights_only = True))\nmodel.eval()\n\n# Evaluate on test set\ncorrect_test = 0\ntotal_test = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        correct_test += torch.sum(preds == labels)\n        total_test += labels.size(0)\n\ntest_accuracy = correct_test.double() / total_test\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:14:11.537339Z","iopub.execute_input":"2025-02-03T14:14:11.537650Z","iopub.status.idle":"2025-02-03T14:40:48.072659Z","shell.execute_reply.started":"2025-02-03T14:14:11.537625Z","shell.execute_reply":"2025-02-03T14:40:48.071559Z"}},"outputs":[{"name":"stdout","text":"3454906\nEpoch 1/35\nTrain Loss: 0.5471, Train Acc: 0.7050, Train F1: 0.7013\nVal Loss: 0.3750, Val Acc: 0.8207, Val F1: 0.8195\nEpoch 2/35\nTrain Loss: 0.4787, Train Acc: 0.7568, Train F1: 0.7568\nVal Loss: 0.3380, Val Acc: 0.8384, Val F1: 0.8382\nEpoch 3/35\nTrain Loss: 0.4524, Train Acc: 0.7797, Train F1: 0.7799\nVal Loss: 0.3318, Val Acc: 0.8535, Val F1: 0.8538\nEpoch 4/35\nTrain Loss: 0.4223, Train Acc: 0.7970, Train F1: 0.7973\nVal Loss: 0.3241, Val Acc: 0.8384, Val F1: 0.8380\nEpoch 5/35\nTrain Loss: 0.4076, Train Acc: 0.8025, Train F1: 0.8029\nVal Loss: 0.3122, Val Acc: 0.8611, Val F1: 0.8611\nEpoch 6/35\nTrain Loss: 0.3891, Train Acc: 0.8141, Train F1: 0.8145\nVal Loss: 0.2851, Val Acc: 0.8687, Val F1: 0.8688\nEpoch 7/35\nTrain Loss: 0.3653, Train Acc: 0.8207, Train F1: 0.8211\nVal Loss: 0.2647, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 8/35\nTrain Loss: 0.3610, Train Acc: 0.8293, Train F1: 0.8296\nVal Loss: 0.2919, Val Acc: 0.8813, Val F1: 0.8812\nEpoch 9/35\nTrain Loss: 0.3589, Train Acc: 0.8282, Train F1: 0.8286\nVal Loss: 0.3180, Val Acc: 0.8662, Val F1: 0.8664\nEpoch 10/35\nTrain Loss: 0.3451, Train Acc: 0.8365, Train F1: 0.8368\nVal Loss: 0.2722, Val Acc: 0.8813, Val F1: 0.8814\nEpoch 11/35\nTrain Loss: 0.3315, Train Acc: 0.8433, Train F1: 0.8436\nVal Loss: 0.2387, Val Acc: 0.8889, Val F1: 0.8891\nLearning rate decreased from 0.000100 to 0.000010\nEpoch 12/35\nTrain Loss: 0.3100, Train Acc: 0.8560, Train F1: 0.8563\nVal Loss: 0.2333, Val Acc: 0.9066, Val F1: 0.9067\nEpoch 13/35\nTrain Loss: 0.3101, Train Acc: 0.8553, Train F1: 0.8556\nVal Loss: 0.2549, Val Acc: 0.8939, Val F1: 0.8938\nEpoch 14/35\nTrain Loss: 0.3050, Train Acc: 0.8580, Train F1: 0.8583\nVal Loss: 0.2336, Val Acc: 0.9015, Val F1: 0.9016\nEpoch 15/35\nTrain Loss: 0.2971, Train Acc: 0.8575, Train F1: 0.8577\nVal Loss: 0.2535, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 16/35\nTrain Loss: 0.3054, Train Acc: 0.8555, Train F1: 0.8558\nVal Loss: 0.2441, Val Acc: 0.8965, Val F1: 0.8963\nLearning rate decreased from 0.000010 to 0.000001\nEpoch 17/35\nTrain Loss: 0.2878, Train Acc: 0.8697, Train F1: 0.8699\nVal Loss: 0.2391, Val Acc: 0.8990, Val F1: 0.8991\nEpoch 18/35\nTrain Loss: 0.2949, Train Acc: 0.8595, Train F1: 0.8598\nVal Loss: 0.2375, Val Acc: 0.9015, Val F1: 0.9016\nEpoch 19/35\nTrain Loss: 0.2955, Train Acc: 0.8627, Train F1: 0.8630\nVal Loss: 0.2415, Val Acc: 0.9066, Val F1: 0.9066\nEpoch 20/35\nTrain Loss: 0.2935, Train Acc: 0.8629, Train F1: 0.8632\nVal Loss: 0.2300, Val Acc: 0.9040, Val F1: 0.9042\nEpoch 21/35\nTrain Loss: 0.2906, Train Acc: 0.8653, Train F1: 0.8656\nVal Loss: 0.2424, Val Acc: 0.8965, Val F1: 0.8964\nEpoch 22/35\nTrain Loss: 0.2926, Train Acc: 0.8622, Train F1: 0.8625\nVal Loss: 0.2342, Val Acc: 0.9040, Val F1: 0.9041\nEpoch 23/35\nTrain Loss: 0.2929, Train Acc: 0.8631, Train F1: 0.8634\nVal Loss: 0.2323, Val Acc: 0.9040, Val F1: 0.9042\nEpoch 24/35\nTrain Loss: 0.2954, Train Acc: 0.8643, Train F1: 0.8645\nVal Loss: 0.2361, Val Acc: 0.9015, Val F1: 0.9016\nEpoch 25/35\nTrain Loss: 0.2949, Train Acc: 0.8646, Train F1: 0.8649\nVal Loss: 0.2370, Val Acc: 0.9015, Val F1: 0.9016\nEpoch 26/35\nTrain Loss: 0.2928, Train Acc: 0.8632, Train F1: 0.8634\nVal Loss: 0.2354, Val Acc: 0.9015, Val F1: 0.9014\nEpoch 27/35\nTrain Loss: 0.2885, Train Acc: 0.8659, Train F1: 0.8661\nVal Loss: 0.2383, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 28/35\nTrain Loss: 0.2848, Train Acc: 0.8668, Train F1: 0.8671\nVal Loss: 0.2411, Val Acc: 0.9091, Val F1: 0.9091\nEpoch 29/35\nTrain Loss: 0.2953, Train Acc: 0.8654, Train F1: 0.8657\nVal Loss: 0.2337, Val Acc: 0.9066, Val F1: 0.9066\nEpoch 30/35\nTrain Loss: 0.2912, Train Acc: 0.8679, Train F1: 0.8681\nVal Loss: 0.2307, Val Acc: 0.9015, Val F1: 0.9015\nEpoch 31/35\nTrain Loss: 0.2927, Train Acc: 0.8651, Train F1: 0.8654\nVal Loss: 0.2275, Val Acc: 0.9015, Val F1: 0.9017\nEpoch 32/35\nTrain Loss: 0.2915, Train Acc: 0.8623, Train F1: 0.8626\nVal Loss: 0.2337, Val Acc: 0.8990, Val F1: 0.8989\nEpoch 33/35\nTrain Loss: 0.2936, Train Acc: 0.8621, Train F1: 0.8624\nVal Loss: 0.2389, Val Acc: 0.8990, Val F1: 0.8990\nEpoch 34/35\nTrain Loss: 0.2890, Train Acc: 0.8645, Train F1: 0.8648\nVal Loss: 0.2382, Val Acc: 0.8965, Val F1: 0.8965\nEpoch 35/35\nTrain Loss: 0.2877, Train Acc: 0.8668, Train F1: 0.8671\nVal Loss: 0.2345, Val Acc: 0.9066, Val F1: 0.9066\nBest Validation Accuracy: 0.9091\nTest Accuracy: 0.8758\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Experiment 7 [ EfficientNet + SeResNet + CBAM] ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nfrom torch.nn import init\n\nclass CBAM(nn.Module):\n    def __init__(self, channels, reduction_ratio=16):\n        super(CBAM, self).__init__()\n        self.channel_attention = ChannelAttention(channels, reduction_ratio)\n        self.spatial_attention = SpatialAttention()\n\n    def forward(self, x):\n        x = self.channel_attention(x)\n        x = self.spatial_attention(x)\n        return x\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, channels, reduction_ratio):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        \n        self.fc = nn.Sequential(\n            conv1x1(channels, channels // reduction_ratio, bias=True),\n            nn.ReLU(inplace=True),\n            conv1x1(channels // reduction_ratio, channels, bias=True)\n        )\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = self.sigmoid(avg_out + max_out)\n        return x * out\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        self.conv = conv3x3_block(\n            in_channels=2,\n            out_channels=1,\n            activation=None\n        )\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        out = torch.cat([avg_out, max_out], dim=1)\n        out = self.conv(out)\n        out = self.sigmoid(out)\n        return x * out\n\nclass HybridBlock(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 stride,\n                 expansion_factor=4,\n                 use_cbam=True):\n        super(HybridBlock, self).__init__()\n        \n        mid_channels = in_channels * expansion_factor\n        self.use_cbam = use_cbam\n        self.residual = (in_channels == out_channels) and (stride == 1)\n        self.pw_expand = conv1x1_block(\n            in_channels=in_channels,\n            out_channels=mid_channels,\n            activation=\"swish\")\n        \n        self.dw_conv = dwconv3x3_block(\n            in_channels=mid_channels,\n            out_channels=mid_channels,\n            stride=stride,\n            activation=\"swish\")\n\n        self.se = SEBlock(\n            channels=mid_channels,\n            reduction=4,\n            mid_activation=\"swish\")\n\n        if self.use_cbam:\n            self.cbam = CBAM(mid_channels)\n\n        self.pw_project = conv1x1_block(\n            in_channels=mid_channels,\n            out_channels=out_channels,\n            activation=None)\n\n    def forward(self, x):\n        if self.residual:\n            identity = x\n        \n        x = self.pw_expand(x)\n        x = self.dw_conv(x)\n        x = self.se(x)\n        \n        if self.use_cbam:\n            x = self.cbam(x)\n            \n        x = self.pw_project(x)\n        \n        if self.residual:\n            x = x + identity\n            \n        return x\n\nclass LightHybridNet(nn.Module):\n\n    def __init__(self,\n                 channels=[16, 24, 48, 88, 168],\n                 init_block_channels=24,\n                 final_block_channels=1280,\n                 num_classes=1000):\n        super(LightHybridNet, self).__init__()\n        \n        self.features = nn.Sequential()\n\n        self.features.add_module(\"init_block\", conv3x3_block(\n            in_channels=3,\n            out_channels=init_block_channels,\n            stride=2,\n            activation=\"swish\"))\n        \n        in_channels = init_block_channels\n\n        for i, out_channels in enumerate(channels):\n            stage = nn.Sequential()\n            stride = 2 if i > 0 else 1\n            use_cbam = (i >= len(channels) - 2) \n            \n            stage.add_module(\"unit1\", HybridBlock(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                stride=stride,\n                use_cbam=use_cbam))\n            \n            if i >= 1: \n                stage.add_module(\"unit2\", HybridBlock(\n                    in_channels=out_channels,\n                    out_channels=out_channels,\n                    stride=1,\n                    use_cbam=use_cbam))\n            \n            in_channels = out_channels\n            self.features.add_module(f\"stage{i + 1}\", stage)\n\n        self.features.add_module(\"final_block\", conv1x1_block(\n            in_channels=in_channels,\n            out_channels=final_block_channels,\n            activation=\"swish\"))\n        \n        self.features.add_module(\"final_pool\", nn.AdaptiveAvgPool2d(1))\n\n        self.output = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(\n                in_features=final_block_channels,\n                out_features=num_classes)\n        )\n\n        self._init_params()\n\n    def _init_params(self):\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Conv2d):\n                init.kaiming_normal_(module.weight)\n                if module.bias is not None:\n                    init.constant_(module.bias, 0)\n            elif isinstance(module, nn.BatchNorm2d):\n                init.constant_(module.weight, 1)\n                init.constant_(module.bias, 0)\n            elif isinstance(module, nn.Linear):\n                init.normal_(module.weight, std=0.01)\n                if module.bias is not None:\n                    init.constant_(module.bias, 0)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.output(x)\n        return x\n\ndef create_model_7(num_classes=1000, **kwargs):\n    return LightHybridNet(num_classes=num_classes, **kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:24:51.521373Z","iopub.execute_input":"2025-02-04T04:24:51.521741Z","iopub.status.idle":"2025-02-04T04:24:51.538055Z","shell.execute_reply.started":"2025-02-04T04:24:51.521707Z","shell.execute_reply":"2025-02-04T04:24:51.537218Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Initialize model\nmodel = create_model_7(num_classes=2)  # Set to 2 classes\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(num_params)\n\n# Transfer to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Setup optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.1, patience=3\n)\n\n# Calculate class weights\nclass_counts = train_new_df['label_encoded'].value_counts()\nclass_weights = torch.FloatTensor([1/class_counts[0], 1/class_counts[1]]).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Train the model\ntrain_model(\n    model=model, \n    train_loader=train_loader, \n    val_loader=val_loader, \n    criterion=criterion, \n    optimizer=optimizer, \n    num_epochs=35\n)\n\n# Load the best model\nmodel.load_state_dict(torch.load('best_model.pth', weights_only = True))\nmodel.eval()\n\n# Evaluate on test set\ncorrect_test = 0\ntotal_test = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        correct_test += torch.sum(preds == labels)\n        total_test += labels.size(0)\n\ntest_accuracy = correct_test.double() / total_test\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:36:29.865864Z","iopub.execute_input":"2025-02-04T04:36:29.866173Z","iopub.status.idle":"2025-02-04T05:04:29.959049Z","shell.execute_reply.started":"2025-02-04T04:36:29.866148Z","shell.execute_reply":"2025-02-04T05:04:29.957969Z"}},"outputs":[{"name":"stdout","text":"1188828\nEpoch 1/35\nTrain Loss: 0.5729, Train Acc: 0.6905, Train F1: 0.6912\nVal Loss: 0.4035, Val Acc: 0.8056, Val F1: 0.8046\nEpoch 2/35\nTrain Loss: 0.4952, Train Acc: 0.7450, Train F1: 0.7447\nVal Loss: 0.3791, Val Acc: 0.8131, Val F1: 0.8135\nEpoch 3/35\nTrain Loss: 0.4754, Train Acc: 0.7627, Train F1: 0.7627\nVal Loss: 0.3688, Val Acc: 0.8182, Val F1: 0.8182\nEpoch 4/35\nTrain Loss: 0.4609, Train Acc: 0.7707, Train F1: 0.7709\nVal Loss: 0.3454, Val Acc: 0.8207, Val F1: 0.8207\nEpoch 5/35\nTrain Loss: 0.4493, Train Acc: 0.7746, Train F1: 0.7747\nVal Loss: 0.3359, Val Acc: 0.8409, Val F1: 0.8412\nEpoch 6/35\nTrain Loss: 0.4327, Train Acc: 0.7873, Train F1: 0.7876\nVal Loss: 0.3171, Val Acc: 0.8510, Val F1: 0.8513\nEpoch 7/35\nTrain Loss: 0.4275, Train Acc: 0.7864, Train F1: 0.7867\nVal Loss: 0.3209, Val Acc: 0.8561, Val F1: 0.8560\nEpoch 8/35\nTrain Loss: 0.4138, Train Acc: 0.7964, Train F1: 0.7968\nVal Loss: 0.3002, Val Acc: 0.8586, Val F1: 0.8589\nEpoch 9/35\nTrain Loss: 0.3973, Train Acc: 0.8026, Train F1: 0.8029\nVal Loss: 0.2900, Val Acc: 0.8611, Val F1: 0.8609\nEpoch 10/35\nTrain Loss: 0.3977, Train Acc: 0.8059, Train F1: 0.8062\nVal Loss: 0.2905, Val Acc: 0.8611, Val F1: 0.8614\nEpoch 11/35\nTrain Loss: 0.3854, Train Acc: 0.8143, Train F1: 0.8146\nVal Loss: 0.3002, Val Acc: 0.8712, Val F1: 0.8714\nEpoch 12/35\nTrain Loss: 0.3811, Train Acc: 0.8111, Train F1: 0.8114\nVal Loss: 0.2994, Val Acc: 0.8687, Val F1: 0.8687\nEpoch 13/35\nTrain Loss: 0.3719, Train Acc: 0.8153, Train F1: 0.8156\nVal Loss: 0.3002, Val Acc: 0.8636, Val F1: 0.8631\nEpoch 14/35\nTrain Loss: 0.3704, Train Acc: 0.8234, Train F1: 0.8237\nVal Loss: 0.2920, Val Acc: 0.8662, Val F1: 0.8662\nEpoch 15/35\nTrain Loss: 0.3676, Train Acc: 0.8220, Train F1: 0.8223\nVal Loss: 0.2921, Val Acc: 0.8737, Val F1: 0.8739\nEpoch 16/35\nTrain Loss: 0.3631, Train Acc: 0.8294, Train F1: 0.8297\nVal Loss: 0.2872, Val Acc: 0.8763, Val F1: 0.8758\nEpoch 17/35\nTrain Loss: 0.3671, Train Acc: 0.8293, Train F1: 0.8296\nVal Loss: 0.2993, Val Acc: 0.8763, Val F1: 0.8758\nEpoch 18/35\nTrain Loss: 0.3516, Train Acc: 0.8311, Train F1: 0.8314\nVal Loss: 0.2883, Val Acc: 0.8662, Val F1: 0.8664\nEpoch 19/35\nTrain Loss: 0.3460, Train Acc: 0.8347, Train F1: 0.8351\nVal Loss: 0.3928, Val Acc: 0.8359, Val F1: 0.8347\nEpoch 20/35\nTrain Loss: 0.3408, Train Acc: 0.8388, Train F1: 0.8392\nVal Loss: 0.3455, Val Acc: 0.8232, Val F1: 0.8218\nLearning rate decreased from 0.000100 to 0.000010\nEpoch 21/35\nTrain Loss: 0.3313, Train Acc: 0.8442, Train F1: 0.8445\nVal Loss: 0.2962, Val Acc: 0.8611, Val F1: 0.8610\nEpoch 22/35\nTrain Loss: 0.3296, Train Acc: 0.8444, Train F1: 0.8448\nVal Loss: 0.2927, Val Acc: 0.8687, Val F1: 0.8686\nEpoch 23/35\nTrain Loss: 0.3273, Train Acc: 0.8473, Train F1: 0.8476\nVal Loss: 0.2859, Val Acc: 0.8662, Val F1: 0.8664\nEpoch 24/35\nTrain Loss: 0.3188, Train Acc: 0.8533, Train F1: 0.8536\nVal Loss: 0.3178, Val Acc: 0.8636, Val F1: 0.8639\nLearning rate decreased from 0.000010 to 0.000001\nEpoch 25/35\nTrain Loss: 0.3220, Train Acc: 0.8516, Train F1: 0.8519\nVal Loss: 0.3056, Val Acc: 0.8586, Val F1: 0.8586\nEpoch 26/35\nTrain Loss: 0.3254, Train Acc: 0.8515, Train F1: 0.8518\nVal Loss: 0.3195, Val Acc: 0.8384, Val F1: 0.8378\nEpoch 27/35\nTrain Loss: 0.3261, Train Acc: 0.8463, Train F1: 0.8466\nVal Loss: 0.3037, Val Acc: 0.8485, Val F1: 0.8481\nEpoch 28/35\nTrain Loss: 0.3238, Train Acc: 0.8469, Train F1: 0.8473\nVal Loss: 0.2977, Val Acc: 0.8636, Val F1: 0.8637\nEpoch 29/35\nTrain Loss: 0.3247, Train Acc: 0.8463, Train F1: 0.8466\nVal Loss: 0.3011, Val Acc: 0.8535, Val F1: 0.8535\nEpoch 30/35\nTrain Loss: 0.3239, Train Acc: 0.8462, Train F1: 0.8466\nVal Loss: 0.2886, Val Acc: 0.8611, Val F1: 0.8611\nEpoch 31/35\nTrain Loss: 0.3202, Train Acc: 0.8503, Train F1: 0.8507\nVal Loss: 0.3039, Val Acc: 0.8561, Val F1: 0.8560\nEpoch 32/35\nTrain Loss: 0.3193, Train Acc: 0.8486, Train F1: 0.8490\nVal Loss: 0.3095, Val Acc: 0.8460, Val F1: 0.8457\nEpoch 33/35\nTrain Loss: 0.3162, Train Acc: 0.8517, Train F1: 0.8520\nVal Loss: 0.3072, Val Acc: 0.8510, Val F1: 0.8507\nEpoch 34/35\nTrain Loss: 0.3197, Train Acc: 0.8513, Train F1: 0.8516\nVal Loss: 0.2995, Val Acc: 0.8687, Val F1: 0.8688\nEpoch 35/35\nTrain Loss: 0.3182, Train Acc: 0.8512, Train F1: 0.8515\nVal Loss: 0.2990, Val Acc: 0.8561, Val F1: 0.8558\nBest Validation Accuracy: 0.8763\nTest Accuracy: 0.8515\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Experinent 8 [ EfficientNet + SeResNet]","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nfrom torch.nn import init\n\nclass AdaptiveSEBlock(nn.Module):\n    def __init__(self, channels, min_reduction=4, max_reduction=16):\n        super(AdaptiveSEBlock, self).__init__()\n        reduction = min(max(min_reduction, channels // 8), max_reduction)\n        \n        mid_channels = max(channels // reduction, 8)\n        \n        self.gate = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, mid_channels, 1),\n            nn.SiLU(inplace=True),\n            nn.Conv2d(mid_channels, channels, 1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        return x * self.gate(x)\n\nclass LightweightConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, groups=1):\n        super(LightweightConvBlock, self).__init__()\n        padding = kernel_size // 2\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels),\n            nn.BatchNorm2d(in_channels),\n            nn.SiLU(inplace=True),\n            nn.Conv2d(in_channels, out_channels, 1),\n            nn.BatchNorm2d(out_channels),\n            nn.SiLU(inplace=True)\n        )\n        \n    def forward(self, x):\n        return self.conv(x)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels, kernel_size=3):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = LightweightConvBlock(channels, channels, kernel_size)\n        self.se = AdaptiveSEBlock(channels)\n        self.conv2 = LightweightConvBlock(channels, channels, kernel_size)\n        \n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.se(out)\n        out = self.conv2(out)\n        return out + identity\n\nclass DownsampleBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super(DownsampleBlock, self).__init__()\n        self.conv = LightweightConvBlock(in_channels, out_channels, kernel_size, stride=2)\n        self.se = AdaptiveSEBlock(out_channels)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.se(x)\n        return x\n\nclass LightHybridNet(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, base_channels=32):\n        super(LightHybridNet, self).__init__()\n        \n        # Initial convolution\n        self.init_conv = nn.Sequential(\n            nn.Conv2d(input_channels, base_channels, 3, stride=2, padding=1),\n            nn.BatchNorm2d(base_channels),\n            nn.SiLU(inplace=True)\n        )\n        \n        # Main architecture\n        self.stage1 = self._make_stage(base_channels, base_channels * 2)\n        self.stage2 = self._make_stage(base_channels * 2, base_channels * 4)\n        self.stage3 = self._make_stage(base_channels * 4, base_channels * 8)\n        \n        # Global features\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.dropout = nn.Dropout(0.2)\n        \n\n        self.classifier = nn.Sequential(\n            nn.Linear(base_channels * 8, base_channels * 4),\n            nn.SiLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(base_channels * 4, num_classes)\n        )\n        \n        self._initialize_weights()\n        \n    def _make_stage(self, in_channels, out_channels):\n        return nn.Sequential(\n            DownsampleBlock(in_channels, out_channels),\n            ResidualBlock(out_channels),\n            ResidualBlock(out_channels)\n        )\n        \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n                \n    def forward(self, x):\n        x = self.init_conv(x)\n        \n        x1 = self.stage1(x)\n        x2 = self.stage2(x1)\n        x3 = self.stage3(x2)\n\n        x = self.global_pool(x3)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        \n        return x\n\ndef create_model_8(num_classes=1000, **kwargs):\n    return LightHybridNet(num_classes=num_classes, **kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T05:04:29.960286Z","iopub.execute_input":"2025-02-04T05:04:29.960564Z","iopub.status.idle":"2025-02-04T05:04:29.976047Z","shell.execute_reply.started":"2025-02-04T05:04:29.960540Z","shell.execute_reply":"2025-02-04T05:04:29.975143Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Initialize model\nmodel = create_model_8(num_classes=2)  \nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(num_params)\n\n# Transfer to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Setup optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.1, patience=3\n)\n\n# Calculate class weights\nclass_counts = train_new_df['label_encoded'].value_counts()\nclass_weights = torch.FloatTensor([1/class_counts[0], 1/class_counts[1]]).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Train the model\ntrain_model(\n    model=model, \n    train_loader=train_loader, \n    val_loader=val_loader, \n    criterion=criterion, \n    optimizer=optimizer, \n    num_epochs=35\n)\n\n# Load the best model\nmodel.load_state_dict(torch.load('best_model.pth', weights_only = True))\nmodel.eval()\n\n# Evaluate on test set\ncorrect_test = 0\ntotal_test = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        correct_test += torch.sum(preds == labels)\n        total_test += labels.size(0)\n\ntest_accuracy = correct_test.double() / total_test\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T05:04:29.977453Z","iopub.execute_input":"2025-02-04T05:04:29.977656Z","iopub.status.idle":"2025-02-04T05:30:15.856696Z","shell.execute_reply.started":"2025-02-04T05:04:29.977639Z","shell.execute_reply":"2025-02-04T05:30:15.855523Z"}},"outputs":[{"name":"stdout","text":"487330\nEpoch 1/35\nTrain Loss: 0.5628, Train Acc: 0.6923, Train F1: 0.6929\nVal Loss: 0.3606, Val Acc: 0.8131, Val F1: 0.8120\nEpoch 2/35\nTrain Loss: 0.4621, Train Acc: 0.7718, Train F1: 0.7716\nVal Loss: 0.3336, Val Acc: 0.8333, Val F1: 0.8337\nEpoch 3/35\nTrain Loss: 0.4366, Train Acc: 0.7888, Train F1: 0.7890\nVal Loss: 0.3100, Val Acc: 0.8434, Val F1: 0.8437\nEpoch 4/35\nTrain Loss: 0.4143, Train Acc: 0.7984, Train F1: 0.7987\nVal Loss: 0.2920, Val Acc: 0.8611, Val F1: 0.8614\nEpoch 5/35\nTrain Loss: 0.4017, Train Acc: 0.8033, Train F1: 0.8036\nVal Loss: 0.2988, Val Acc: 0.8561, Val F1: 0.8560\nEpoch 6/35\nTrain Loss: 0.3847, Train Acc: 0.8151, Train F1: 0.8154\nVal Loss: 0.2978, Val Acc: 0.8662, Val F1: 0.8664\nEpoch 7/35\nTrain Loss: 0.3820, Train Acc: 0.8104, Train F1: 0.8107\nVal Loss: 0.2909, Val Acc: 0.8636, Val F1: 0.8636\nEpoch 8/35\nTrain Loss: 0.3791, Train Acc: 0.8199, Train F1: 0.8202\nVal Loss: 0.3150, Val Acc: 0.8561, Val F1: 0.8558\nEpoch 9/35\nTrain Loss: 0.3669, Train Acc: 0.8251, Train F1: 0.8254\nVal Loss: 0.2778, Val Acc: 0.8737, Val F1: 0.8740\nEpoch 10/35\nTrain Loss: 0.3618, Train Acc: 0.8320, Train F1: 0.8324\nVal Loss: 0.2965, Val Acc: 0.8687, Val F1: 0.8690\nEpoch 11/35\nTrain Loss: 0.3505, Train Acc: 0.8324, Train F1: 0.8327\nVal Loss: 0.2942, Val Acc: 0.8737, Val F1: 0.8737\nEpoch 12/35\nTrain Loss: 0.3516, Train Acc: 0.8375, Train F1: 0.8378\nVal Loss: 0.2588, Val Acc: 0.8737, Val F1: 0.8739\nEpoch 13/35\nTrain Loss: 0.3430, Train Acc: 0.8391, Train F1: 0.8394\nVal Loss: 0.2611, Val Acc: 0.8737, Val F1: 0.8739\nLearning rate decreased from 0.000100 to 0.000010\nEpoch 14/35\nTrain Loss: 0.3342, Train Acc: 0.8429, Train F1: 0.8433\nVal Loss: 0.2459, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 15/35\nTrain Loss: 0.3301, Train Acc: 0.8499, Train F1: 0.8502\nVal Loss: 0.2586, Val Acc: 0.8889, Val F1: 0.8891\nEpoch 16/35\nTrain Loss: 0.3234, Train Acc: 0.8461, Train F1: 0.8464\nVal Loss: 0.2508, Val Acc: 0.8864, Val F1: 0.8866\nEpoch 17/35\nTrain Loss: 0.3190, Train Acc: 0.8477, Train F1: 0.8480\nVal Loss: 0.2454, Val Acc: 0.8838, Val F1: 0.8841\nEpoch 18/35\nTrain Loss: 0.3276, Train Acc: 0.8469, Train F1: 0.8472\nVal Loss: 0.2515, Val Acc: 0.8889, Val F1: 0.8890\nLearning rate decreased from 0.000010 to 0.000001\nEpoch 19/35\nTrain Loss: 0.3212, Train Acc: 0.8452, Train F1: 0.8455\nVal Loss: 0.2524, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 20/35\nTrain Loss: 0.3200, Train Acc: 0.8519, Train F1: 0.8522\nVal Loss: 0.2497, Val Acc: 0.8889, Val F1: 0.8891\nEpoch 21/35\nTrain Loss: 0.3265, Train Acc: 0.8432, Train F1: 0.8435\nVal Loss: 0.2504, Val Acc: 0.8864, Val F1: 0.8866\nEpoch 22/35\nTrain Loss: 0.3252, Train Acc: 0.8440, Train F1: 0.8443\nVal Loss: 0.2518, Val Acc: 0.8838, Val F1: 0.8841\nEpoch 23/35\nTrain Loss: 0.3221, Train Acc: 0.8473, Train F1: 0.8476\nVal Loss: 0.2518, Val Acc: 0.8737, Val F1: 0.8740\nEpoch 24/35\nTrain Loss: 0.3247, Train Acc: 0.8480, Train F1: 0.8483\nVal Loss: 0.2527, Val Acc: 0.8813, Val F1: 0.8815\nEpoch 25/35\nTrain Loss: 0.3276, Train Acc: 0.8473, Train F1: 0.8476\nVal Loss: 0.2554, Val Acc: 0.8813, Val F1: 0.8816\nEpoch 26/35\nTrain Loss: 0.3286, Train Acc: 0.8470, Train F1: 0.8474\nVal Loss: 0.2570, Val Acc: 0.8889, Val F1: 0.8888\nEpoch 27/35\nTrain Loss: 0.3191, Train Acc: 0.8504, Train F1: 0.8507\nVal Loss: 0.2529, Val Acc: 0.8838, Val F1: 0.8841\nEpoch 28/35\nTrain Loss: 0.3233, Train Acc: 0.8493, Train F1: 0.8496\nVal Loss: 0.2545, Val Acc: 0.8889, Val F1: 0.8891\nEpoch 29/35\nTrain Loss: 0.3158, Train Acc: 0.8529, Train F1: 0.8532\nVal Loss: 0.2522, Val Acc: 0.8788, Val F1: 0.8790\nEpoch 30/35\nTrain Loss: 0.3202, Train Acc: 0.8503, Train F1: 0.8507\nVal Loss: 0.2498, Val Acc: 0.8788, Val F1: 0.8790\nEpoch 31/35\nTrain Loss: 0.3161, Train Acc: 0.8532, Train F1: 0.8535\nVal Loss: 0.2539, Val Acc: 0.8813, Val F1: 0.8815\nEpoch 32/35\nTrain Loss: 0.3211, Train Acc: 0.8524, Train F1: 0.8527\nVal Loss: 0.2540, Val Acc: 0.8763, Val F1: 0.8765\nEpoch 33/35\nTrain Loss: 0.3251, Train Acc: 0.8443, Train F1: 0.8446\nVal Loss: 0.2583, Val Acc: 0.8889, Val F1: 0.8891\nEpoch 34/35\nTrain Loss: 0.3170, Train Acc: 0.8544, Train F1: 0.8547\nVal Loss: 0.2527, Val Acc: 0.8813, Val F1: 0.8816\nEpoch 35/35\nTrain Loss: 0.3173, Train Acc: 0.8532, Train F1: 0.8535\nVal Loss: 0.2482, Val Acc: 0.8838, Val F1: 0.8841\nBest Validation Accuracy: 0.8939\nTest Accuracy: 0.8576\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Experiment 9 [EfficientNet + SeResNet + SqueezeNet + CBAM]","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nfrom torch.nn import functional as F\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, channels, reduction_ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        \n        self.fc = nn.Sequential(\n            nn.Conv2d(channels, channels // reduction_ratio, 1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels // reduction_ratio, channels, 1, bias=False)\n        )\n        \n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = torch.sigmoid(avg_out + max_out)\n        return out\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        \n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        out = torch.cat([avg_out, max_out], dim=1)\n        out = torch.sigmoid(self.conv(out))\n        return out\n\nclass CBAMBlock(nn.Module):\n    def __init__(self, channels, reduction_ratio=16, kernel_size=7):\n        super(CBAMBlock, self).__init__()\n        self.channel_attention = ChannelAttention(channels, reduction_ratio)\n        self.spatial_attention = SpatialAttention(kernel_size)\n        \n    def forward(self, x):\n        x = x * self.channel_attention(x)\n        x = x * self.spatial_attention(x)\n        return x\n\nclass LightweightFireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand_channels):\n        super(LightweightFireModule, self).__init__()\n        self.squeeze = conv1x1_block(\n            in_channels=in_channels,\n            out_channels=squeeze_channels,\n            activation=\"relu\")\n            \n        self.expand1x1 = conv1x1_block(\n            in_channels=squeeze_channels,\n            out_channels=expand_channels//2,\n            activation=\"relu\")\n            \n        self.expand3x3 = conv3x3_block(\n            in_channels=squeeze_channels,\n            out_channels=expand_channels//2,\n            activation=\"relu\")\n            \n        self.cbam = CBAMBlock(expand_channels)\n        \n    def forward(self, x):\n        x = self.squeeze(x)\n        x1 = self.expand1x1(x)\n        x2 = self.expand3x3(x)\n        out = torch.cat([x1, x2], dim=1)\n        out = self.cbam(out)\n        return out\n\nclass EfficientBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, expansion_factor=6):\n        super(EfficientBlock, self).__init__()\n        self.residual = (in_channels == out_channels) and (stride == 1)\n        mid_channels = in_channels * expansion_factor\n        \n        self.expand = conv1x1_block(\n            in_channels=in_channels,\n            out_channels=mid_channels,\n            activation=\"swish\") if expansion_factor != 1 else None\n            \n        self.depthwise = dwconv3x3_block(\n            in_channels=mid_channels,\n            out_channels=mid_channels,\n            stride=stride,\n            activation=\"swish\")\n            \n        self.se = SEBlock(\n            channels=mid_channels,\n            reduction=4,\n            mid_activation=\"swish\")\n            \n        self.project = conv1x1_block(\n            in_channels=mid_channels,\n            out_channels=out_channels,\n            activation=None)\n            \n    def forward(self, x):\n        if self.residual:\n            identity = x\n        if self.expand is not None:\n            x = self.expand(x)\n        x = self.depthwise(x)\n        x = self.se(x)\n        x = self.project(x)\n        if self.residual:\n            x = x + identity\n        return x\n\nclass LightFusionNet(nn.Module):\n    def __init__(self,\n                 in_channels=3,\n                 num_classes=1000,\n                 init_channels=32):\n        super(LightFusionNet, self).__init__()\n        \n        self.init_block = conv3x3_block(\n            in_channels=in_channels,\n            out_channels=init_channels,\n            stride=2)\n\n        self.stage1 = nn.Sequential(\n            LightweightFireModule(init_channels, 16, 64),\n            LightweightFireModule(64, 16, 64)\n        )\n        \n        self.stage2 = nn.Sequential(\n            EfficientBlock(64, 128, stride=2, expansion_factor=4),\n            EfficientBlock(128, 128, expansion_factor=4)\n        )\n        \n        self.stage3 = nn.Sequential(\n            EfficientBlock(128, 256, stride=2, expansion_factor=6),\n            EfficientBlock(256, 256, expansion_factor=6)\n        )\n        \n        self.final_block = conv1x1_block(\n            in_channels=256,\n            out_channels=512,\n            activation=\"swish\")\n            \n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(0.2)\n        self.output = nn.Linear(512, num_classes)\n        \n        self._init_params()\n        \n    def _init_params(self):\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Conv2d):\n                init.kaiming_uniform_(module.weight)\n                if module.bias is not None:\n                    init.constant_(module.bias, 0)\n            elif isinstance(module, nn.BatchNorm2d):\n                init.constant_(module.weight, 1)\n                init.constant_(module.bias, 0)\n            elif isinstance(module, nn.Linear):\n                init.normal_(module.weight, 0, 0.01)\n                init.constant_(module.bias, 0)\n                \n    def forward(self, x):\n        x = self.init_block(x)\n        identity1 = x\n        x = self.stage1(x)\n        if x.size() == identity1.size():\n            x = x + identity1\n        x = self.stage2(x)\n        x = self.stage3(x)\n        \n        x = self.final_block(x)\n        x = self.global_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.output(x)\n        return x\n\ndef create_model_9(num_classes=1000, **kwargs):\n    return LightFusionNet(num_classes=num_classes, **kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T05:30:15.858244Z","iopub.execute_input":"2025-02-04T05:30:15.858538Z","iopub.status.idle":"2025-02-04T05:30:15.877435Z","shell.execute_reply.started":"2025-02-04T05:30:15.858513Z","shell.execute_reply":"2025-02-04T05:30:15.876506Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Initialize model\nmodel = create_model_9(num_classes=2) \nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(num_params)\n\n# Transfer to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Setup optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.1, patience=3\n)\n\n# Calculate class weights\nclass_counts = train_new_df['label_encoded'].value_counts()\nclass_weights = torch.FloatTensor([1/class_counts[0], 1/class_counts[1]]).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Train the model\ntrain_model(\n    model=model, \n    train_loader=train_loader, \n    val_loader=val_loader, \n    criterion=criterion, \n    optimizer=optimizer, \n    num_epochs=35\n)\n\n# Load the best model\nmodel.load_state_dict(torch.load('best_model.pth', weights_only = True))\nmodel.eval()\n\n# Evaluate on test set\ncorrect_test = 0\ntotal_test = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        correct_test += torch.sum(preds == labels)\n        total_test += labels.size(0)\n\ntest_accuracy = correct_test.double() / total_test\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T05:30:15.878309Z","iopub.execute_input":"2025-02-04T05:30:15.878543Z","iopub.status.idle":"2025-02-04T06:29:30.370016Z","shell.execute_reply.started":"2025-02-04T05:30:15.878517Z","shell.execute_reply":"2025-02-04T06:29:30.368980Z"}},"outputs":[{"name":"stdout","text":"3092646\nEpoch 1/35\nTrain Loss: 0.5326, Train Acc: 0.7298, Train F1: 0.7300\nVal Loss: 0.4001, Val Acc: 0.8131, Val F1: 0.8134\nEpoch 2/35\nTrain Loss: 0.4913, Train Acc: 0.7513, Train F1: 0.7511\nVal Loss: 0.3630, Val Acc: 0.8308, Val F1: 0.8310\nEpoch 3/35\nTrain Loss: 0.4595, Train Acc: 0.7736, Train F1: 0.7736\nVal Loss: 0.3246, Val Acc: 0.8409, Val F1: 0.8411\nEpoch 4/35\nTrain Loss: 0.4282, Train Acc: 0.7892, Train F1: 0.7893\nVal Loss: 0.2945, Val Acc: 0.8485, Val F1: 0.8487\nEpoch 5/35\nTrain Loss: 0.4166, Train Acc: 0.7999, Train F1: 0.8002\nVal Loss: 0.3048, Val Acc: 0.8662, Val F1: 0.8664\nEpoch 6/35\nTrain Loss: 0.3993, Train Acc: 0.8079, Train F1: 0.8082\nVal Loss: 0.2723, Val Acc: 0.8889, Val F1: 0.8888\nEpoch 7/35\nTrain Loss: 0.3867, Train Acc: 0.8148, Train F1: 0.8151\nVal Loss: 0.2804, Val Acc: 0.8586, Val F1: 0.8578\nEpoch 8/35\nTrain Loss: 0.3833, Train Acc: 0.8170, Train F1: 0.8173\nVal Loss: 0.2645, Val Acc: 0.8838, Val F1: 0.8841\nEpoch 9/35\nTrain Loss: 0.3684, Train Acc: 0.8245, Train F1: 0.8248\nVal Loss: 0.2538, Val Acc: 0.8889, Val F1: 0.8891\nEpoch 10/35\nTrain Loss: 0.3590, Train Acc: 0.8281, Train F1: 0.8284\nVal Loss: 0.2580, Val Acc: 0.8838, Val F1: 0.8838\nLearning rate decreased from 0.000100 to 0.000010\nEpoch 11/35\nTrain Loss: 0.3444, Train Acc: 0.8371, Train F1: 0.8375\nVal Loss: 0.2348, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 12/35\nTrain Loss: 0.3378, Train Acc: 0.8419, Train F1: 0.8422\nVal Loss: 0.2353, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 13/35\nTrain Loss: 0.3439, Train Acc: 0.8399, Train F1: 0.8402\nVal Loss: 0.2338, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 14/35\nTrain Loss: 0.3453, Train Acc: 0.8389, Train F1: 0.8392\nVal Loss: 0.2467, Val Acc: 0.8939, Val F1: 0.8939\nEpoch 15/35\nTrain Loss: 0.3319, Train Acc: 0.8448, Train F1: 0.8451\nVal Loss: 0.2335, Val Acc: 0.8914, Val F1: 0.8916\nLearning rate decreased from 0.000010 to 0.000001\nEpoch 16/35\nTrain Loss: 0.3367, Train Acc: 0.8404, Train F1: 0.8408\nVal Loss: 0.2329, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 17/35\nTrain Loss: 0.3339, Train Acc: 0.8459, Train F1: 0.8462\nVal Loss: 0.2348, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 18/35\nTrain Loss: 0.3363, Train Acc: 0.8412, Train F1: 0.8416\nVal Loss: 0.2318, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 19/35\nTrain Loss: 0.3273, Train Acc: 0.8440, Train F1: 0.8443\nVal Loss: 0.2314, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 20/35\nTrain Loss: 0.3315, Train Acc: 0.8436, Train F1: 0.8440\nVal Loss: 0.2319, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 21/35\nTrain Loss: 0.3305, Train Acc: 0.8478, Train F1: 0.8481\nVal Loss: 0.2299, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 22/35\nTrain Loss: 0.3319, Train Acc: 0.8449, Train F1: 0.8452\nVal Loss: 0.2369, Val Acc: 0.8965, Val F1: 0.8965\nEpoch 23/35\nTrain Loss: 0.3288, Train Acc: 0.8444, Train F1: 0.8448\nVal Loss: 0.2321, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 24/35\nTrain Loss: 0.3370, Train Acc: 0.8411, Train F1: 0.8415\nVal Loss: 0.2283, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 25/35\nTrain Loss: 0.3309, Train Acc: 0.8452, Train F1: 0.8455\nVal Loss: 0.2349, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 26/35\nTrain Loss: 0.3345, Train Acc: 0.8411, Train F1: 0.8415\nVal Loss: 0.2346, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 27/35\nTrain Loss: 0.3321, Train Acc: 0.8446, Train F1: 0.8450\nVal Loss: 0.2320, Val Acc: 0.8990, Val F1: 0.8991\nEpoch 28/35\nTrain Loss: 0.3335, Train Acc: 0.8448, Train F1: 0.8451\nVal Loss: 0.2396, Val Acc: 0.8990, Val F1: 0.8990\nEpoch 29/35\nTrain Loss: 0.3314, Train Acc: 0.8434, Train F1: 0.8437\nVal Loss: 0.2412, Val Acc: 0.8965, Val F1: 0.8964\nEpoch 30/35\nTrain Loss: 0.3327, Train Acc: 0.8436, Train F1: 0.8440\nVal Loss: 0.2316, Val Acc: 0.8914, Val F1: 0.8916\nEpoch 31/35\nTrain Loss: 0.3384, Train Acc: 0.8451, Train F1: 0.8454\nVal Loss: 0.2351, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 32/35\nTrain Loss: 0.3302, Train Acc: 0.8469, Train F1: 0.8472\nVal Loss: 0.2371, Val Acc: 0.8965, Val F1: 0.8965\nEpoch 33/35\nTrain Loss: 0.3285, Train Acc: 0.8468, Train F1: 0.8471\nVal Loss: 0.2313, Val Acc: 0.8939, Val F1: 0.8941\nEpoch 34/35\nTrain Loss: 0.3285, Train Acc: 0.8482, Train F1: 0.8485\nVal Loss: 0.2334, Val Acc: 0.8965, Val F1: 0.8966\nEpoch 35/35\nTrain Loss: 0.3295, Train Acc: 0.8473, Train F1: 0.8476\nVal Loss: 0.2306, Val Acc: 0.8965, Val F1: 0.8967\nBest Validation Accuracy: 0.8990\nTest Accuracy: 0.8636\n","output_type":"stream"}],"execution_count":21}]}